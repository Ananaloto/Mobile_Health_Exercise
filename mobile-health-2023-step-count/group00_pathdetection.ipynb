{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# This is the template for the submission. If you want, you can develop your algorithm in a regular Python script and copy the code here for submission.\n",
    "\n",
    "# Team members (e-mail, legi):\n",
    "# zhisun@ethz.ch, 22-958-227\n",
    "# enjcao@ethz.ch, 22-942-700\n",
    "# yifzhou@ethz.ch, 22-940-381"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from Lilygo.Recording import Recording\n",
    "from Lilygo.Dataset import Dataset\n",
    "\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.stats import entropy\n",
    "from scipy.signal import welch\n",
    "from scipy.fftpack import fft\n",
    "\n",
    "from Lilygo.Recording import Recording, data_integrity\n",
    "from Lilygo.Dataset import Dataset\n",
    "\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "import joblib"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local dirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the path of all traces\n",
    "dir_data = 'E:\\\\Sunzhichao\\\\ETHz\\\\2223Spring\\\\Mobile_Health\\\\data\\\\'\n",
    "dir_traces_train = dir_data + 'train\\\\'\n",
    "dir_traces_test = dir_data + 'test\\\\'\n",
    "dir_labels = dir_data + 'labels\\\\'\n",
    "dir_loaded = dir_data + 'Loaded_data\\\\'\n",
    "dir_models = dir_data + 'models\\\\'\n",
    "# recorded\n",
    "dir_recorded = 'data/recorded'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Get the path of all traces\n",
    "# dir_traces = '/kaggle/input/mobile-health-2023-path-detection/data/test'\n",
    "dir_traces = dir_traces_test\n",
    "filenames = [join(dir_traces, f) for f in listdir(dir_traces) if (isfile(join(dir_traces, f)) and f[-5:] == '.json')]\n",
    "filenames.sort()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function aims to find the component caused by gravity from data, which means the signal around 0 Hz\n",
    "def get_gravity(data):\n",
    "    filtered_data = np.zeros_like(data)\n",
    "    # Parameters in IIR filter\n",
    "    alpla = [1, -1.979133761292768, 0.979521463540373]\n",
    "    beta = [0.000086384997973502, 0.00012769995947004, 0.000086384997973502]\n",
    "    # Formula of IIR filter\n",
    "    for i in range(2, len(data)):\n",
    "        filtered_data[i] = alpla[0] * (data[i] * beta[0] + data[i-1] * beta[1] + data[i-2] * beta[2] - filtered_data[i-1] * alpla[1] - filtered_data[i-2] * alpla[2])\n",
    "    return filtered_data\n",
    "\n",
    "# This function aims to realize a low-pass filter with cutoff frequency = 1 Hz. Because according to massive amounts of data, the general \n",
    "# minimum frequency of human walking is about 1 Hz\n",
    "def get_highpass(data):\n",
    "    filtered_data = np.zeros_like(data)  # filtered_data\n",
    "    alpla = [1, -1.905384612118461, 0.910092542787947]\n",
    "    beta = [0.953986986993339, -1.907503180919730, 0.953986986993339]\n",
    "    for i in range(2, len(data)):\n",
    "        filtered_data[i] = alpla[0] * (data[i] * beta[0] + data[i-1] * beta[1] + data[i-2] * beta[2] - filtered_data[i-1] * alpla[1] - filtered_data[i-2] * alpla[2])\n",
    "    return filtered_data\n",
    "\n",
    "# This funciton aims to realize a high-pass filter with cutoff frequency = 5 Hz. Because according to massive amounts of data, the general \n",
    "# maximum frequency of human walking is about 5 Hz\n",
    "def get_lowpass(data):\n",
    "    filtered_data = np.zeros_like(data)  # filtered_data\n",
    "    alpla = [1, -1.80898117793047, 0.827224480562408]\n",
    "    beta = [0.096665967120306, -0.172688631608676, 0.095465967120306]\n",
    "    for i in range(2, len(data)):\n",
    "        filtered_data[i] = alpla[0] * (data[i] * beta[0] + data[i-1] * beta[1] + data[i-2] * beta[2] - filtered_data[i-1] * alpla[1] - filtered_data[i-2] * alpla[2])\n",
    "    return filtered_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute energy\n",
    "def compute_energy(data):\n",
    "    window_length = len(data)  # length of each window\n",
    "    # Define the window function (e.g., Hanning or Hamming)\n",
    "    window = np.hanning(window_length)\n",
    "    # Apply the window function to the data\n",
    "    windowed_data = data * window\n",
    "    # Compute the FFT of the windowed data\n",
    "    fft_result = np.fft.fft(windowed_data)\n",
    "    # Compute the squared magnitudes of the FFT components (excluding DC)\n",
    "    mag_squared = np.abs(fft_result[1:window_length // 2])**2\n",
    "    # Sum the squared magnitudes and normalize by the window length\n",
    "    energy = np.sum(mag_squared) / window_length\n",
    "    # fft_data now contains the energy feature for each window of data\n",
    "    return energy\n",
    "\n",
    "# compute entropy\n",
    "def compute_entropy(data):\n",
    "    # Assume we have accelerometer data in a 3D array called \"data\", where each row\n",
    "    # represents a window of data and each column represents a sensor axis (x, y, z or magnitute)\n",
    "    window_length = len(data) # length of each window\n",
    "    # Define the window function (e.g., Hanning or Hamming)\n",
    "    window = np.hanning(window_length)\n",
    "    # Apply the window function to the data\n",
    "    windowed_data = data * window\n",
    "    # Compute the FFT of the windowed data\n",
    "    fft_result = np.fft.fft(windowed_data)\n",
    "    # Compute the magnitudes of the FFT components (excluding DC)\n",
    "    mag = np.abs(fft_result[1:window_length // 2])\n",
    "    # Normalize the magnitudes to obtain a probability distribution\n",
    "    mag_norm = mag / np.sum(mag)\n",
    "    # Compute the information entropy of the probability distribution\n",
    "    entropy = -np.sum(mag_norm * np.log2(mag_norm))\n",
    "    # fft_data now contains the frequency-domain entropy feature for each window of data\n",
    "    return entropy\n",
    "\n",
    "def get_features(data):\n",
    "    # get features from sliding windows\n",
    "    dc = np.mean(data)\n",
    "    energy = compute_energy(np.asarray(data))\n",
    "    entropy = compute_entropy(np.asarray(data))\n",
    "    return dc, energy, entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_and_extract_features(trace, window_size=60, sampling_rate=50):\n",
    "    \"\"\"\n",
    "    Preprocess the data and extract features from the 3D accelerometer, gyroscope, and magnetometer data.\n",
    "\n",
    "    Args:\n",
    "    trace (Lilygo.Recording.Recording): Object containing the raw data with accelerometer data stored in lists (e.g. trace.data['ax'])\n",
    "    window_size (int): The window size in seconds for splitting the data\n",
    "    sampling_rate (int): The sampling rate of the data in Hz\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: A DataFrame with the extracted features and location labels\n",
    "    \"\"\"\n",
    "    # Read data from trace\n",
    "    # To-Do: filter raw data with implemented function\n",
    "    ax = get_lowpass(get_highpass(trace.data['ax'].values))\n",
    "    ay = get_lowpass(get_highpass(trace.data['ay'].values))\n",
    "    az = get_lowpass(get_highpass(trace.data['az'].values))\n",
    "    \n",
    "    '''gx = trace.data['gx'].values\n",
    "    gy = trace.data['gy'].values\n",
    "    gz = trace.data['gz'].values\n",
    "    \n",
    "    mx = trace.data['mx'].values\n",
    "    my = trace.data['my'].values\n",
    "    mz = trace.data['mz'].values'''\n",
    "    \n",
    "    # Compute the length of each window in samples\n",
    "    window_samples = window_size * sampling_rate\n",
    "    \n",
    "    # Compute the number of windows in the recording\n",
    "    num_windows = len(ax) // window_samples\n",
    "\n",
    "    # Initialize lists for storing extracted features and location labels\n",
    "    features = []\n",
    "    loc_labels = []\n",
    "\n",
    "    # Helper function to compute the magnitude of a vector\n",
    "    magnitude = lambda vec: np.sqrt(np.sum(vec**2, axis=1))\n",
    "\n",
    "    for i in range(num_windows):\n",
    "       \n",
    "        # Extract the accelerometer, gyroscope, and magnetometer data for the current window\n",
    "        acc_data = np.array([ax[i*window_samples:(i+1)*window_samples],\n",
    "                             ay[i*window_samples:(i+1)*window_samples],\n",
    "                             az[i*window_samples:(i+1)*window_samples]]).T\n",
    "        '''gyro_data = np.array([gx[i*window_samples:(i+1)*window_samples],\n",
    "                              gy[i*window_samples:(i+1)*window_samples],\n",
    "                              gz[i*window_samples:(i+1)*window_samples]]).T\n",
    "        mag_data = np.array([mx[i*window_samples:(i+1)*window_samples],\n",
    "                             my[i*window_samples:(i+1)*window_samples],\n",
    "                             mz[i*window_samples:(i+1)*window_samples]]).T'''\n",
    "        \n",
    "        '''figure,ax = plt.subplots(3, 1, figsize=(10, 6))\n",
    "        ax[0].plot(acc_data[:,0])\n",
    "        ax[0].set_ylabel('ax')\n",
    "        ax[1].plot(acc_data[:,1])\n",
    "        ax[1].set_ylabel('ay')\n",
    "        ax[2].plot(acc_data[:,2])\n",
    "        ax[2].set_ylabel('az')'''\n",
    "\n",
    "        # Compute magnitudes\n",
    "        acc_magnitude = np.sqrt(np.sum(acc_data**2, axis=1))\n",
    "        #gyro_magnitude = magnitude(gyro_data)\n",
    "        #mag_magnitude = magnitude(mag_data)\n",
    "\n",
    "        # Calculate features (according to https://www.sciencedirect.com/science/article/pii/S1574119211001222)\n",
    "        '''ax_amp = acc_data[np.argmax(acc_data[:,0]), 0] - acc_data[np.argmin(acc_data[:,0]), 0]\n",
    "        ay_amp = acc_data[np.argmax(acc_data[:,1]), 1] - acc_data[np.argmin(acc_data[:,1]), 1]\n",
    "        az_amp = acc_data[np.argmax(acc_data[:,2]), 2] - acc_data[np.argmin(acc_data[:,2]), 2]'''\n",
    "        \n",
    "        ax_amp = abs(np.mean(acc_data[:,0]))\n",
    "        ay_amp = abs(np.mean(acc_data[:,1]))\n",
    "        az_amp = abs(np.mean(acc_data[:,2]))\n",
    "        a_amp_list = [ax_amp, ay_amp, az_amp]\n",
    "        a_amp_list.sort() # Sorting list of numbers in ascending\n",
    "        #print('ax_amp:',ax_amp,'ay_amp:',ay_amp,'az_amp',az_amp)\n",
    "        A = a_amp_list[2] #Feature A: the maximum amplitude among all dimensions (represents motion range for location)\n",
    "        B = a_amp_list[2]/a_amp_list[1] # Feature B and C: ratio of the maximum amplitudes in different axes (represents DoF in movement for location)\n",
    "        C = a_amp_list[2]/a_amp_list[0]\n",
    "        #print('A:',A,'B:',B,'C',C)\n",
    "\n",
    "        # Calculate the energy and entropy of acc_mag in the frequency domain (D and F)\n",
    "        freq, Pxx = welch(acc_magnitude, fs=sampling_rate) # use of the fast Fourier transform for the estimation of power spectra\n",
    "        #plt.plot(freq,Pxx)\n",
    "        \n",
    "        D = np.max(Pxx) # Feature D: the maximum energy captured by the accelerator\n",
    "        F = np.sum(Pxx) # Feature F: the overall energy captured by the accelerator\n",
    "        norm_Pxx = Pxx / F # normalize the power spectrum\n",
    "        E = entropy(norm_Pxx) # Feature E: normalized information entropy of the discrete FFT component magnitudes\n",
    "\n",
    "   \n",
    "        # Append the features to the list\n",
    "        features.append([A, B, C, D, E, F])\n",
    "        \n",
    "        # Calculate the timestamp for the current window as the median of the timestamps (not necessary for location)\n",
    "        # timestamp = np.median(trace.timestamp[i*window_samples:(i+1)*window_samples]) \n",
    "\n",
    "        # Determine the location label for the current window based on the timestamp\n",
    "        # loc_label = trace.labels.get('board_loc')\n",
    "\n",
    "        # Append the label to the labels list\n",
    "        # loc_labels.append(loc_label)\n",
    "    #plt.show()\n",
    "\n",
    "    # Create a DataFrame with the extracted features and location labels\n",
    "    features_df = pd.DataFrame(features, columns=['A', 'B', 'C', 'D', 'E', 'F'])\n",
    "    # features_df['loc_label'] = loc_labels\n",
    "    #print(features_df)\n",
    "\n",
    "    return features_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\szzcc\\.conda\\envs\\mhealth23\\lib\\site-packages\\sklearn\\base.py:338: UserWarning: Trying to unpickle estimator SVC from version 1.2.2 when using version 1.0.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/modules/model_persistence.html#security-maintainability-limitations\n",
      "  UserWarning,\n",
      "c:\\Users\\szzcc\\.conda\\envs\\mhealth23\\lib\\site-packages\\sklearn\\base.py:338: UserWarning: Trying to unpickle estimator GridSearchCV from version 1.2.2 when using version 1.0.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/modules/model_persistence.html#security-maintainability-limitations\n",
      "  UserWarning,\n"
     ]
    }
   ],
   "source": [
    "# Location\n",
    "location_model = joblib.load('./trained_models/location_svm_model.joblib')\n",
    "activity_xgb_model = joblib.load('trained_models/activity_xgboost_model_feature.joblib')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process(data):\n",
    "    # Find the component caused by gravity from data and remove it from the singanl\n",
    "    data_gravity = get_gravity(data)\n",
    "    data_user = data - data_gravity\n",
    "    # Get user's acceleration along the gravity direction by dot product\n",
    "    data_acc = data_user * data_gravity\n",
    "    # Add low pass and high pass filter to reduce noise in signal (possible human walking rate:1 - 5Hz)\n",
    "    data_filtered = get_highpass(data_acc)\n",
    "    data_filtered = get_lowpass(data_filtered)\n",
    "    return data_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_segment(trace):\n",
    "    # Calculate raw magnitude of accelerometer signal\n",
    "    amagn_acc = [np.sqrt(a**2+trace.data['ay'].values[i]**2+trace.data['az'].values[i]**2)for i, a in enumerate(trace.data['ay'].values)]\n",
    "    # Pre-process data\n",
    "    amagn = pre_process(amagn_acc)\n",
    "    # Calculate window size\n",
    "    sampling_rate = 200\n",
    "    std_win = 3 #s\n",
    "    window_size = round(std_win*sampling_rate)\n",
    "    segment_trace = [amagn[s:s+window_size] for s in range(0, len(amagn)-window_size, round(window_size/2))]\n",
    "    return segment_trace\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_watch_loc(trace):\n",
    "    # # Get features of data\n",
    "    # features_df = preprocess_and_extract_features(trace)\n",
    "\n",
    "    # # Prepare data for classification\n",
    "    # scaler = StandardScaler()\n",
    "    # X_test = scaler.fit_transform(features_df)\n",
    "\n",
    "    # # Predict the location with loaded model\n",
    "    # y_pred = location_model.predict(X_test)\n",
    "    # y_pred = np.squeeze(y_pred)\n",
    "    # y_final = np.argmax(np.bincount(y_pred.astype(int)))\n",
    "\n",
    "    y_final = 3\n",
    "    \n",
    "    return y_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_path_idx(trace):\n",
    "    return 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_step_count(trace):\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_activity(trace):\n",
    "    stand, walk, run, cycle = 0, 0, 0, 0\n",
    "    \n",
    "    # get segment data\n",
    "    segment_trace = get_segment(trace)\n",
    "    # add feature extraction\n",
    "    num_features = 3\n",
    "    featured_trace = np.zeros((np.shape(segment_trace)[0], num_features))\n",
    "    for i in range(np.shape(segment_trace)[0]):\n",
    "        featured_trace[i,] = get_features(segment_trace[i])\n",
    "    # Create the XGBoost DMatrix object for the test data\n",
    "    dtest = xgb.DMatrix(featured_trace)\n",
    "\n",
    "    # Make predictions on the test set and evaluate the model\n",
    "    y_pred = activity_xgb_model.predict(dtest)\n",
    "    \n",
    "    # filter prediction by 60s\n",
    "    # Sliding window: 60s\n",
    "    std_win = 10\n",
    "    n = round (60 / std_win * 2 - 1)\n",
    "    y_pred_count = np.zeros(4)\n",
    "    \n",
    "    y_pred_60 = y_pred.copy()\n",
    "    for s in range(0, len(y_pred) - n, int(n/2+1)):\n",
    "        # window = 60s \n",
    "        windowed_label = y_pred[s : s+n]\n",
    "        for j in range(n): \n",
    "            # Find the label that appears the most\n",
    "            for k in range(4):\n",
    "                if windowed_label[j] == k:\n",
    "                    y_pred_count[k]+=1\n",
    "        label_argmax = np.where(y_pred_count == np.max(y_pred_count))\n",
    "        # print(label_argmax)\n",
    "        if len(label_argmax)==1:\n",
    "            y_pred_60[s : s+n] = np.argmax(y_pred_count)\n",
    "    \n",
    "    # remove duplicated elements\n",
    "    predicted = list(set(y_pred_60))\n",
    "    if 0 in predicted:\n",
    "        stand = 1\n",
    "    if 1 in predicted:\n",
    "        walk = 1\n",
    "    if 2 in predicted:\n",
    "        run = 1\n",
    "    if 3 in predicted:\n",
    "        cycle = 1\n",
    "\n",
    "    return stand, walk, run, cycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process traces:  10 / 376\n",
      "Process traces:  20 / 376\n",
      "Process traces:  30 / 376\n",
      "Process traces:  40 / 376\n"
     ]
    }
   ],
   "source": [
    "# Loop through all traces and calculate the step count for each trace\n",
    "solution_file = []\n",
    "for idx, filename in enumerate(filenames):\n",
    "    trace = Recording(filename, no_labels=True, mute=True)\n",
    "    categorization_results = {'watch_loc': 0, 'path_idx': 0, 'step_count': 0, 'stand': 0, 'walk': 0, 'run': 0, 'cycle': 0}\n",
    "\n",
    "    #\n",
    "    # Your algorithm goes here\n",
    "    # Make sure, you do not use the gps data and are tolerant for missing data (see task set).\n",
    "    # Your program must not crash when single smartphone data traces are missing.\n",
    "    #\n",
    "\n",
    "    stand, walk, run, cycle = get_activity(trace)\n",
    "    categorization_results['watch_loc'] = get_watch_loc(trace)\n",
    "    categorization_results['path_idx'] = get_path_idx(trace)\n",
    "    categorization_results['step_count'] = get_step_count(trace)\n",
    "    categorization_results['stand'], categorization_results['walk'], categorization_results['run'], categorization_results['cycle'] = get_activity(trace)\n",
    "\n",
    "\n",
    "    # Append your calculated results and the id of each trace and category to the solution file\n",
    "    trace_id = ''.join([*filename][-8:-5])\n",
    "    for counter_label, category in enumerate(categorization_results):\n",
    "        solution_file.append([trace_id + f'_{counter_label+1}', categorization_results[category]])\n",
    "    # show progress\n",
    "    if (idx+1)%10 == 0:\n",
    "        print(\"Process traces: \", idx+1, '/', len(filenames))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Write the detected step counts into a .csv file to then upload the .csv file to Kaggle\n",
    "# When cross-checking the .csv file on your computer, we recommend using the text editor and NOT excel so that the results are displayed correctly\n",
    "# IMPORTANT: Do NOT change the name of the columns ('Id' and 'Category') of the .csv file\n",
    "submission_file_df = pd.DataFrame(np.asarray(solution_file), columns=['Id', 'Category'])\n",
    "submission_file_df.to_csv('results/submission_2.csv', header=['Id', 'Category'], index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
