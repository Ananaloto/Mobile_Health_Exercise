{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# This is the template for the submission. If you want, you can develop your algorithm in a regular Python script and copy the code here for submission.\n",
    "\n",
    "# Team members (e-mail, legi):\n",
    "# zhisun@ethz.ch, 22-958-227\n",
    "# enjcao@ethz.ch, 22-942-700\n",
    "# yifzhou@ethz.ch, 22-940-381"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from Lilygo.Recording import Recording\n",
    "from Lilygo.Dataset import Dataset\n",
    "\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.stats import entropy\n",
    "from scipy.signal import welch\n",
    "from scipy.fftpack import fft\n",
    "\n",
    "from Lilygo.Recording import Recording, data_integrity\n",
    "from Lilygo.Dataset import Dataset\n",
    "\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Get the path of all traces\n",
    "dir_traces = '/kaggle/input/mobile-health-2023-path-detection/data/test'\n",
    "filenames = [join(dir_traces, f) for f in listdir(dir_traces) if isfile(join(dir_traces, f))]\n",
    "filenames.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_highpass(data):\n",
    "    filtered_data = np.zeros_like(data)  # filtered_data\n",
    "    alpla = [1, -1.905384612118461, 0.910092542787947]\n",
    "    beta = [0.953986986993339, -1.907503180919730, 0.953986986993339]\n",
    "\n",
    "    for i in range(2, len(data)):\n",
    "        filtered_data[i] = alpla[0] * (data[i] * beta[0] + data[i-1] * beta[1] + data[i-2] * beta[2] - filtered_data[i-1] * alpla[1] - filtered_data[i-2] * alpla[2])\n",
    "    return filtered_data\n",
    "\n",
    "# This funciton aims to realize a high-pass filter with cutoff frequency = 5 Hz. Because according to massive amounts of data, the general \n",
    "# maximum frequency of human walking is about 5 Hz\n",
    "def get_lowpass(data):\n",
    "    filtered_data = np.zeros_like(data)  # filtered_data\n",
    "    alpla = [1, -1.80898117793047, 0.827224480562408]\n",
    "    beta = [0.096665967120306, -0.172688631608676, 0.095465967120306]\n",
    "\n",
    "    for i in range(2, len(data)):\n",
    "        filtered_data[i] = alpla[0] * (data[i] * beta[0] + data[i-1] * beta[1] + data[i-2] * beta[2] - filtered_data[i-1] * alpla[1] - filtered_data[i-2] * alpla[2])\n",
    "    return filtered_data\n",
    "\n",
    "def preprocess_and_extract_features(trace, window_size=60, sampling_rate=50):\n",
    "    \"\"\"\n",
    "    Preprocess the data and extract features from the 3D accelerometer, gyroscope, and magnetometer data.\n",
    "\n",
    "    Args:\n",
    "    trace (Lilygo.Recording.Recording): Object containing the raw data with accelerometer data stored in lists (e.g. trace.data['ax'])\n",
    "    window_size (int): The window size in seconds for splitting the data\n",
    "    sampling_rate (int): The sampling rate of the data in Hz\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: A DataFrame with the extracted features and location labels\n",
    "    \"\"\"\n",
    "    # Read data from trace\n",
    "    # To-Do: filter raw data with implemented function\n",
    "    ax = get_lowpass(get_highpass(trace.data['ax'].values))\n",
    "    ay = get_lowpass(get_highpass(trace.data['ay'].values))\n",
    "    az = get_lowpass(get_highpass(trace.data['az'].values))\n",
    "    \n",
    "    '''gx = trace.data['gx'].values\n",
    "    gy = trace.data['gy'].values\n",
    "    gz = trace.data['gz'].values\n",
    "    \n",
    "    mx = trace.data['mx'].values\n",
    "    my = trace.data['my'].values\n",
    "    mz = trace.data['mz'].values'''\n",
    "    \n",
    "    # Compute the length of each window in samples\n",
    "    window_samples = window_size * sampling_rate\n",
    "    \n",
    "    # Compute the number of windows in the recording\n",
    "    num_windows = len(ax) // window_samples\n",
    "\n",
    "    # Initialize lists for storing extracted features and location labels\n",
    "    features = []\n",
    "    loc_labels = []\n",
    "\n",
    "    # Helper function to compute the magnitude of a vector\n",
    "    magnitude = lambda vec: np.sqrt(np.sum(vec**2, axis=1))\n",
    "\n",
    "    for i in range(num_windows):\n",
    "       \n",
    "        # Extract the accelerometer, gyroscope, and magnetometer data for the current window\n",
    "        acc_data = np.array([ax[i*window_samples:(i+1)*window_samples],\n",
    "                             ay[i*window_samples:(i+1)*window_samples],\n",
    "                             az[i*window_samples:(i+1)*window_samples]]).T\n",
    "        '''gyro_data = np.array([gx[i*window_samples:(i+1)*window_samples],\n",
    "                              gy[i*window_samples:(i+1)*window_samples],\n",
    "                              gz[i*window_samples:(i+1)*window_samples]]).T\n",
    "        mag_data = np.array([mx[i*window_samples:(i+1)*window_samples],\n",
    "                             my[i*window_samples:(i+1)*window_samples],\n",
    "                             mz[i*window_samples:(i+1)*window_samples]]).T'''\n",
    "        \n",
    "        '''figure,ax = plt.subplots(3, 1, figsize=(10, 6))\n",
    "        ax[0].plot(acc_data[:,0])\n",
    "        ax[0].set_ylabel('ax')\n",
    "        ax[1].plot(acc_data[:,1])\n",
    "        ax[1].set_ylabel('ay')\n",
    "        ax[2].plot(acc_data[:,2])\n",
    "        ax[2].set_ylabel('az')'''\n",
    "\n",
    "        # Compute magnitudes\n",
    "        acc_magnitude = np.sqrt(np.sum(acc_data**2, axis=1))\n",
    "        #gyro_magnitude = magnitude(gyro_data)\n",
    "        #mag_magnitude = magnitude(mag_data)\n",
    "\n",
    "        # Calculate features (according to https://www.sciencedirect.com/science/article/pii/S1574119211001222)\n",
    "        '''ax_amp = acc_data[np.argmax(acc_data[:,0]), 0] - acc_data[np.argmin(acc_data[:,0]), 0]\n",
    "        ay_amp = acc_data[np.argmax(acc_data[:,1]), 1] - acc_data[np.argmin(acc_data[:,1]), 1]\n",
    "        az_amp = acc_data[np.argmax(acc_data[:,2]), 2] - acc_data[np.argmin(acc_data[:,2]), 2]'''\n",
    "        \n",
    "        ax_amp = abs(np.mean(acc_data[:,0]))\n",
    "        ay_amp = abs(np.mean(acc_data[:,1]))\n",
    "        az_amp = abs(np.mean(acc_data[:,2]))\n",
    "        a_amp_list = [ax_amp, ay_amp, az_amp]\n",
    "        a_amp_list.sort() # Sorting list of numbers in ascending\n",
    "        #print('ax_amp:',ax_amp,'ay_amp:',ay_amp,'az_amp',az_amp)\n",
    "        A = a_amp_list[2] #Feature A: the maximum amplitude among all dimensions (represents motion range for location)\n",
    "        B = a_amp_list[2]/a_amp_list[1] # Feature B and C: ratio of the maximum amplitudes in different axes (represents DoF in movement for location)\n",
    "        C = a_amp_list[2]/a_amp_list[0]\n",
    "        #print('A:',A,'B:',B,'C',C)\n",
    "\n",
    "        # Calculate the energy and entropy of acc_mag in the frequency domain (D and F)\n",
    "        freq, Pxx = welch(acc_magnitude, fs=sampling_rate) # use of the fast Fourier transform for the estimation of power spectra\n",
    "        #plt.plot(freq,Pxx)\n",
    "        \n",
    "        D = np.max(Pxx) # Feature D: the maximum energy captured by the accelerator\n",
    "        F = np.sum(Pxx) # Feature F: the overall energy captured by the accelerator\n",
    "        norm_Pxx = Pxx / F # normalize the power spectrum\n",
    "        E = entropy(norm_Pxx) # Feature E: normalized information entropy of the discrete FFT component magnitudes\n",
    "\n",
    "   \n",
    "        # Append the features to the list\n",
    "        features.append([A, B, C, D, E, F])\n",
    "        \n",
    "        # Calculate the timestamp for the current window as the median of the timestamps (not necessary for location)\n",
    "        # timestamp = np.median(trace.timestamp[i*window_samples:(i+1)*window_samples]) \n",
    "\n",
    "        # Determine the location label for the current window based on the timestamp\n",
    "        loc_label = trace.labels.get('board_loc')\n",
    "\n",
    "        # Append the label to the labels list\n",
    "        loc_labels.append(loc_label)\n",
    "    #plt.show()\n",
    "\n",
    "    # Create a DataFrame with the extracted features and location labels\n",
    "    features_df = pd.DataFrame(features, columns=['A', 'B', 'C', 'D', 'E', 'F'])\n",
    "    features_df['loc_label'] = loc_labels\n",
    "    #print(features_df)\n",
    "\n",
    "    return features_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Location\n",
    "location_model = joblib.load('./trained_models/location_svm_model.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_watch_loc(trace):\n",
    "    # Get features of data\n",
    "    features_df = preprocess_and_extract_features(trace)\n",
    "\n",
    "    # Prepare data for classification\n",
    "    scaler = StandardScaler()\n",
    "    X_test = scaler.fit_transform(features_df)\n",
    "\n",
    "    # Predict the location with loaded model\n",
    "    y_pred = location_model.predict(X_test)\n",
    "    y_pred = np.squeeze(y_pred)\n",
    "    y_final = np.argmax(np.bincount(y_pred.astype(int)))\n",
    "    \n",
    "    return y_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_path_idx(trace):\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_step_count(trace):\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_activity(trace):\n",
    "    stand, walk, run, cycle = 0, 0, 0, 0\n",
    "    return stand, walk, run, cycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Loop through all traces and calculate the step count for each trace\n",
    "solution_file = []\n",
    "for filename in filenames:\n",
    "    trace = Recording(filename, no_labels=True, mute=True)\n",
    "    categorization_results = {'watch_loc': 0, 'path_idx': 0, 'step_count': 0, 'stand': 0, 'walk': 0, 'run': 0, 'cycle': 0}\n",
    "\n",
    "    #\n",
    "    # Your algorithm goes here\n",
    "    # You can access the variable 'watch_loc' in the dictionary 'categorization_results' for example with\n",
    "    # categorization_results['watch_loc'] = 1\n",
    "    # Make sure, you do not use the gps data and are tolerant for missing data (see task set).\n",
    "    # Your program must not crash when single smartphone data traces are missing.\n",
    "    #\n",
    "\n",
    "    stand, walk, run, cycle = get_activity(trace)\n",
    "    categorization_results['watch_loc'] = get_watch_loc(trace)\n",
    "    categorization_results['path_idx'] = get_path_idx(trace)\n",
    "    categorization_results['step_countstep_count'] = get_step_count(trace)\n",
    "    categorization_results['stand'], categorization_results['walk'], categorization_results['run'], categorization_results['cycle'] = get_activity(trace)\n",
    "\n",
    "\n",
    "    # Append your calculated results and the id of each trace and category to the solution file\n",
    "    trace_id = ''.join([*filename][-8:-5])\n",
    "    for counter_label, category in enumerate(categorization_results):\n",
    "        solution_file.append([trace_id + f'_{counter_label+1}', categorization_results[category]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Write the detected step counts into a .csv file to then upload the .csv file to Kaggle\n",
    "# When cross-checking the .csv file on your computer, we recommend using the text editor and NOT excel so that the results are displayed correctly\n",
    "# IMPORTANT: Do NOT change the name of the columns ('Id' and 'Category') of the .csv file\n",
    "submission_file_df = pd.DataFrame(np.asarray(solution_file), columns=['Id', 'Category'])\n",
    "submission_file_df.to_csv('/kaggle/working/submission.csv', header=['Id', 'Category'], index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
