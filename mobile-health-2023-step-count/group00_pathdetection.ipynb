{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# This is the template for the submission. If you want, you can develop your algorithm in a regular Python script and copy the code here for submission.\n",
    "\n",
    "# Team members (e-mail, legi):\n",
    "# zhisun@ethz.ch, 22-958-227\n",
    "# enjcao@ethz.ch, 22-942-700\n",
    "# yifzhou@ethz.ch, 22-940-381"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from Lilygo.Recording import Recording\n",
    "from Lilygo.Dataset import Dataset\n",
    "\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.stats import entropy\n",
    "from scipy import signal\n",
    "from scipy.signal import welch\n",
    "from scipy.fftpack import fft\n",
    "from scipy.stats import entropy, kurtosis, skew\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "from Lilygo.Recording import Recording, data_integrity\n",
    "from Lilygo.Dataset import Dataset\n",
    "\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "import joblib\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local dirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the path of all traces\n",
    "dir_data = 'E:\\\\Sunzhichao\\\\ETHz\\\\2223Spring\\\\Mobile_Health\\\\data\\\\'\n",
    "dir_traces_train = dir_data + 'train\\\\'\n",
    "dir_traces_test = dir_data + 'test\\\\'\n",
    "dir_labels = dir_data + 'labels\\\\'\n",
    "dir_loaded = dir_data + 'Loaded_data\\\\'\n",
    "dir_models = dir_data + 'models\\\\'\n",
    "# recorded\n",
    "dir_recorded = 'data/recorded'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Get the path of all traces\n",
    "# dir_traces = '/kaggle/input/mobile-health-2023-path-detection/data/test'\n",
    "dir_traces = dir_traces_test\n",
    "filenames = [join(dir_traces, f) for f in listdir(dir_traces) if (isfile(join(dir_traces, f)) and f[-5:] == '.json')]\n",
    "filenames.sort()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function aims to find the component caused by gravity from data, which means the signal around 0 Hz\n",
    "def get_gravity(data):\n",
    "    filtered_data = np.zeros_like(data)\n",
    "    # Parameters in IIR filter\n",
    "    alpla = [1, -1.979133761292768, 0.979521463540373]\n",
    "    beta = [0.000086384997973502, 0.00012769995947004, 0.000086384997973502]\n",
    "    # Formula of IIR filter\n",
    "    for i in range(2, len(data)):\n",
    "        filtered_data[i] = alpla[0] * (data[i] * beta[0] + data[i-1] * beta[1] + data[i-2] * beta[2] - filtered_data[i-1] * alpla[1] - filtered_data[i-2] * alpla[2])\n",
    "    return filtered_data\n",
    "\n",
    "# This function aims to realize a low-pass filter with cutoff frequency = 1 Hz. Because according to massive amounts of data, the general \n",
    "# minimum frequency of human walking is about 1 Hz\n",
    "def get_highpass(data):\n",
    "    filtered_data = np.zeros_like(data)  # filtered_data\n",
    "    alpla = [1, -1.905384612118461, 0.910092542787947]\n",
    "    beta = [0.953986986993339, -1.907503180919730, 0.953986986993339]\n",
    "    for i in range(2, len(data)):\n",
    "        filtered_data[i] = alpla[0] * (data[i] * beta[0] + data[i-1] * beta[1] + data[i-2] * beta[2] - filtered_data[i-1] * alpla[1] - filtered_data[i-2] * alpla[2])\n",
    "    return filtered_data\n",
    "\n",
    "# This funciton aims to realize a high-pass filter with cutoff frequency = 5 Hz. Because according to massive amounts of data, the general \n",
    "# maximum frequency of human walking is about 5 Hz\n",
    "def get_lowpass(data):\n",
    "    filtered_data = np.zeros_like(data)  # filtered_data\n",
    "    alpla = [1, -1.80898117793047, 0.827224480562408]\n",
    "    beta = [0.096665967120306, -0.172688631608676, 0.095465967120306]\n",
    "    for i in range(2, len(data)):\n",
    "        filtered_data[i] = alpla[0] * (data[i] * beta[0] + data[i-1] * beta[1] + data[i-2] * beta[2] - filtered_data[i-1] * alpla[1] - filtered_data[i-2] * alpla[2])\n",
    "    return filtered_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute energy\n",
    "def compute_energy(data):\n",
    "    window_length = len(data)  # length of each window\n",
    "    # Define the window function (e.g., Hanning or Hamming)\n",
    "    window = np.hanning(window_length)\n",
    "    # Apply the window function to the data\n",
    "    windowed_data = data * window\n",
    "    # Compute the FFT of the windowed data\n",
    "    fft_result = np.fft.fft(windowed_data)\n",
    "    # Compute the squared magnitudes of the FFT components (excluding DC)\n",
    "    mag_squared = np.abs(fft_result[1:window_length // 2])**2\n",
    "    # Sum the squared magnitudes and normalize by the window length\n",
    "    energy = np.sum(mag_squared) / window_length\n",
    "    # fft_data now contains the energy feature for each window of data\n",
    "    return energy\n",
    "\n",
    "# compute entropy\n",
    "def compute_entropy(data):\n",
    "    # Assume we have accelerometer data in a 3D array called \"data\", where each row\n",
    "    # represents a window of data and each column represents a sensor axis (x, y, z or magnitute)\n",
    "    window_length = len(data) # length of each window\n",
    "    # Define the window function (e.g., Hanning or Hamming)\n",
    "    window = np.hanning(window_length)\n",
    "    # Apply the window function to the data\n",
    "    windowed_data = data * window\n",
    "    # Compute the FFT of the windowed data\n",
    "    fft_result = np.fft.fft(windowed_data)\n",
    "    # Compute the magnitudes of the FFT components (excluding DC)\n",
    "    mag = np.abs(fft_result[1:window_length // 2])\n",
    "    # Normalize the magnitudes to obtain a probability distribution\n",
    "    mag_norm = mag / np.sum(mag)\n",
    "    # Compute the information entropy of the probability distribution\n",
    "    entropy = -np.sum(mag_norm * np.log2(mag_norm))\n",
    "    # fft_data now contains the frequency-domain entropy feature for each window of data\n",
    "    return entropy\n",
    "\n",
    "def get_features_3(data):\n",
    "    # get features from sliding windows\n",
    "    dc = np.mean(data)\n",
    "    energy = compute_energy(np.asarray(data))\n",
    "    entropy = compute_entropy(np.asarray(data))\n",
    "    return dc, energy, entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add new features\n",
    "def reshape_to_windows(x, wl, ol):\n",
    "  \"\"\"\n",
    "  Segments signal into windows by reshaping it into 2D\n",
    "  x: 1-d array to reshape\n",
    "  wl: no. of samples in window\n",
    "  ol: no. of samples overlap\n",
    "  \"\"\"\n",
    "  assert wl>ol, 'Window must be longer than overlap'\n",
    "  step=int(wl-ol)\n",
    "  nrows = int(1+(x.size-wl)//step)\n",
    "  n = int(x.strides[0])\n",
    "  return np.lib.stride_tricks.as_strided(x, shape=(nrows,int(wl)),\n",
    "                                        strides=(step*n,n))\n",
    "\n",
    "def get_AC_DC(data, sr, order = 1, crit_freq = 2):\n",
    "  \"\"\"\n",
    "  returns high-pass and low-pass filtered signals using butterworth filter\n",
    "  data: original windowed signal\n",
    "  sr = sampling frequency\n",
    "  order: order of filter\n",
    "  crit_freq: critical frequency of filter\n",
    "  \"\"\"\n",
    "\n",
    "  sos_low = signal.butter(order, crit_freq, 'lp', fs = sr, output = 'sos')\n",
    "  sos_high = signal.butter(order, crit_freq, 'hp', fs = sr, output = 'sos')\n",
    "  AC = signal.sosfilt(sos_low, data)\n",
    "  DC = signal.sosfilt(sos_high, data)\n",
    "  return AC, DC\n",
    "\n",
    "\n",
    "def extract_temporal_features(data):\n",
    "  \"\"\"\n",
    "  returns time domain features (20) from windowed raw signal:\n",
    "   - mean, standard deviation, kurtosis, skewness;\n",
    "   - RMS, zero-crossing\n",
    "   - The following percentiles: [0, 5, 10, 20, 30, 40, 50, 60, 70, 80, 90, 95, 100] (Q)\n",
    "   - Range: max(x) - min(x)\n",
    "  data: windowed signal\n",
    "  \"\"\"\n",
    "  m = np.mean(data)\n",
    "  sd = np.std(data)\n",
    "  kurt = kurtosis(data)\n",
    "  sk = skew(data)\n",
    "  rms = np.sqrt(np.mean(data**2))\n",
    "  zc = np.sum(np.diff(data>=m))\n",
    "  q = np.array([0, 0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 0.95, 1])\n",
    "  Q = np.quantile(data, q)\n",
    "  range = Q[-1]-Q[0]             \n",
    "  return m, sd, kurt, sk, rms, zc, range, Q[0], Q[1], Q[2], Q[3], Q[4], Q[5], Q[6], Q[7], Q[8], Q[9], Q[10], Q[11], Q[12]\n",
    "\n",
    "\n",
    "def extract_frequential_features(data, sr):\n",
    "  \"\"\"\n",
    "  exctracting frequncy domain features (5) from windowed Fourier transform:\n",
    "   - energy, entropy, centroid, bandwidth, max_freq\n",
    "  data: windowed signal\n",
    "  sf: sampling rate\n",
    "  \"\"\"\n",
    "  window_size = len(data)\n",
    "  data -= np.mean(data)\n",
    "  ft = np.fft.fft(data)/window_size\n",
    "  sr = int(sr)\n",
    "  # get window length\n",
    "  \n",
    "  #discarding mirror part\n",
    "  ft = ft[:window_size//2]\n",
    "  #frequencies of the transofm\n",
    "  freqs = np.fft.fftfreq(window_size, 1/sr)[1:window_size//2]\n",
    "  #the spectral density is the squared of the absolute\n",
    "  Spec = np.abs(ft)**2\n",
    "  #Energy\n",
    "  E = np.sum(Spec)/(window_size//2)\n",
    "  #density\n",
    "  P = Spec[1:]/np.sum(Spec[1:])\n",
    "  #entropy\n",
    "  H = -np.sum(P*np.log2(P))/np.log2((window_size//2))\n",
    "  #centriod \n",
    "  C = np.sum(P*freqs)\n",
    "  #Absolute distance  of frequencies from from Centroid\n",
    "  distC = np.abs((C-freqs))\n",
    "  #bandwidth is the weighted mean of the distance\n",
    "  BW = np.sum(distC*P)\n",
    "  #maximum frequency \n",
    "  max_fr = freqs[np.argmax(Spec[1:])]\n",
    "  return E, H, C, BW, max_fr\n",
    "\n",
    "# 25 features\n",
    "def get_features_25(data):\n",
    "  # AC, DC = get_AC_DC(data, 200)\n",
    "  m, sd, kurt, sk, rms, zc, range, q_0, q_1, q_2, q_3, q_4, q_5, q_6, q_7, q_8, q_9, q_10, q_11, q_12 = extract_temporal_features(data)\n",
    "  E, H, C, BW, max_fr = extract_frequential_features(data, 200)\n",
    "  all_features = [m, sd, kurt, sk, rms, zc, range, q_0, q_1, q_2, q_3, q_4, q_5, q_6, q_7, q_8, q_9, q_10, q_11, q_12, E, H, C, BW, max_fr]\n",
    "  return all_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data pre-processing\n",
    "# This function aims to find the component caused by gravity from data, which means the signal around 0 Hz\n",
    "def get_gravity(data):\n",
    "    filtered_data = np.zeros_like(data)\n",
    "    # Parameters in IIR filter\n",
    "    alpla = [1, -1.979133761292768, 0.979521463540373]\n",
    "    beta = [0.000086384997973502, 0.00012769995947004, 0.000086384997973502]\n",
    "    # Formula of IIR filter\n",
    "    for i in range(2, len(data)):\n",
    "        filtered_data[i] = alpla[0] * (data[i] * beta[0] + data[i-1] * beta[1] + data[i-2] * beta[2] - filtered_data[i-1] * alpla[1] - filtered_data[i-2] * alpla[2])\n",
    "    return filtered_data\n",
    "\n",
    "def get_highpass(data):\n",
    "    filtered_data = np.zeros_like(data)  # filtered_data\n",
    "    alpla = [1, -1.905384612118461, 0.910092542787947]\n",
    "    beta = [0.953986986993339, -1.907503180919730, 0.953986986993339]\n",
    "\n",
    "    for i in range(2, len(data)):\n",
    "        filtered_data[i] = alpla[0] * (data[i] * beta[0] + data[i-1] * beta[1] + data[i-2] * beta[2] - filtered_data[i-1] * alpla[1] - filtered_data[i-2] * alpla[2])\n",
    "    return filtered_data\n",
    "\n",
    "# This funciton aims to realize a high-pass filter with cutoff frequency = 5 Hz. Because according to massive amounts of data, the general \n",
    "# maximum frequency of human walking is about 5 Hz\n",
    "def get_lowpass(data):\n",
    "    filtered_data = np.zeros_like(data)  # filtered_data\n",
    "    alpla = [1, -1.80898117793047, 0.827224480562408]\n",
    "    beta = [0.096665967120306, -0.172688631608676, 0.095465967120306]\n",
    "\n",
    "    for i in range(2, len(data)):\n",
    "        filtered_data[i] = alpla[0] * (data[i] * beta[0] + data[i-1] * beta[1] + data[i-2] * beta[2] - filtered_data[i-1] * alpla[1] - filtered_data[i-2] * alpla[2])\n",
    "    return filtered_data\n",
    "\n",
    "def pre_process(data):\n",
    "    # Find the component caused by gravity from data and remove it from the singanl\n",
    "    data_gravity = get_gravity(data)\n",
    "    data_user = data - data_gravity\n",
    "    # Get user's acceleration along the gravity direction by dot product\n",
    "    data_acc = data_user * data_gravity\n",
    "    # Add low pass and high pass filter to reduce noise in signal (possible human walking rate:1 - 5Hz)\n",
    "    data_filtered = get_highpass(data_acc)\n",
    "    data_filtered = get_lowpass(data_filtered)\n",
    "    return data_filtered\n",
    "\n",
    "def preprocess_and_extract_features(trace, window_size=15, sampling_rate=200):\n",
    "    \"\"\"\n",
    "    Preprocess the data and extract features from the 3D accelerometer, gyroscope, and magnetometer data.\n",
    "\n",
    "    Args:\n",
    "    trace (Lilygo.Recording.Recording): Object containing the raw data with accelerometer data stored in lists (e.g. trace.data['ax'])\n",
    "    window_size (int): The window size in seconds for splitting the data\n",
    "    sampling_rate (int): The sampling rate of the data in Hz\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: A DataFrame with the extracted features and location labels\n",
    "    \"\"\"\n",
    "    # Read data from trace\n",
    "    ax = pre_process(trace.data['ax'].values)\n",
    "    ay = pre_process(trace.data['ay'].values)\n",
    "    az = pre_process(trace.data['az'].values)\n",
    "    \n",
    "    gx = pre_process(trace.data['gx'].values)\n",
    "    gy = pre_process(trace.data['gy'].values)\n",
    "    gz = pre_process(trace.data['gz'].values)\n",
    "    \n",
    "    '''mx = trace.data['mx'].values\n",
    "    my = trace.data['my'].values\n",
    "    mz = trace.data['mz'].values'''\n",
    "    \n",
    "    # Compute the length of each window in samples\n",
    "    window_samples = window_size * sampling_rate\n",
    "    \n",
    "    # Compute the number of windows in the recording\n",
    "    num_windows = len(ax) // window_samples\n",
    "\n",
    "    # Initialize lists for storing extracted features and location labels\n",
    "    features = []\n",
    "    loc_labels = []\n",
    "\n",
    "    # Helper function to compute the magnitude of a vector\n",
    "    magnitude = lambda vec: np.sqrt(np.sum(vec**2, axis=1))\n",
    "\n",
    "    for i in range(num_windows):\n",
    "       \n",
    "        # Extract the accelerometer, gyroscope, and magnetometer data for the current window\n",
    "        acc_data = np.array([ax[i*window_samples:(i+1)*window_samples],\n",
    "                             ay[i*window_samples:(i+1)*window_samples],\n",
    "                             az[i*window_samples:(i+1)*window_samples]]).T\n",
    "        gyro_data = np.array([gx[i*window_samples:(i+1)*window_samples],\n",
    "                              gy[i*window_samples:(i+1)*window_samples],\n",
    "                              gz[i*window_samples:(i+1)*window_samples]]).T\n",
    "        '''mag_data = np.array([mx[i*window_samples:(i+1)*window_samples],\n",
    "                             my[i*window_samples:(i+1)*window_samples],\n",
    "                             mz[i*window_samples:(i+1)*window_samples]]).T'''\n",
    "        \n",
    "        '''figure,ax = plt.subplots(3, 1, figsize=(10, 6))\n",
    "        ax[0].plot(acc_data[:,0])\n",
    "        ax[0].set_ylabel('ax')\n",
    "        ax[1].plot(acc_data[:,1])\n",
    "        ax[1].set_ylabel('ay')\n",
    "        ax[2].plot(acc_data[:,2])\n",
    "        ax[2].set_ylabel('az')'''\n",
    "\n",
    "        # Compute magnitudes\n",
    "        acc_magnitude = np.sqrt(np.sum(acc_data**2, axis=1))\n",
    "        acc_mag_mean = abs(np.mean(acc_magnitude))\n",
    "        acc_mag_std = np.std(acc_magnitude)\n",
    "        #gyro_magnitude = magnitude(gyro_data)\n",
    "        #mag_magnitude = magnitude(mag_data)\n",
    "\n",
    "\n",
    "        # ----ACCELERATOR TIME DOMAIN----\n",
    "        ax_mean = abs(np.mean(acc_data[:,0]))\n",
    "        ay_mean = abs(np.mean(acc_data[:,1]))\n",
    "        az_mean = abs(np.mean(acc_data[:,2]))\n",
    "        a_mean_list = [ax_mean, ay_mean, az_mean]\n",
    "        a_mean_list.sort() # Sorting list of numbers in ascending\n",
    "        Am = a_mean_list[2] # Feature Am: the maximum mean among all dimensions (represents motion range for location)\n",
    "        Bm = a_mean_list[2]/a_mean_list[1] # Feature Bm and Cm: ratio of the maximum mean in different axes (represents DoF in movement for location)\n",
    "        Cm = a_mean_list[2]/a_mean_list[0] \n",
    "\n",
    "        ax_range = acc_data[np.argmax(acc_data[:,0]), 0] - acc_data[np.argmin(acc_data[:,0]), 0]\n",
    "        ay_range = acc_data[np.argmax(acc_data[:,1]), 1] - acc_data[np.argmin(acc_data[:,1]), 1]\n",
    "        az_range = acc_data[np.argmax(acc_data[:,2]), 2] - acc_data[np.argmin(acc_data[:,2]), 2]\n",
    "        a_range_list = [ax_range, ay_range, az_range]\n",
    "        a_range_list.sort() # Sorting list of numbers in ascending\n",
    "        A = a_range_list[2] # Feature A: the maximum range among all dimensions (represents motion range for location)\n",
    "        B = a_range_list[2]/a_range_list[1] # Feature B and C: ratio of the maximum ranges in different axes (represents DoF in movement for location)\n",
    "        C = a_range_list[2]/a_range_list[0] \n",
    "        \n",
    "        \n",
    "        # ----GYROSCOPE TIME DOMAIN----\n",
    "        gx_mean = abs(np.mean(gyro_data[:,0]))\n",
    "        gy_mean = abs(np.mean(gyro_data[:,1]))\n",
    "        gz_mean = abs(np.mean(gyro_data[:,2]))\n",
    "        g_mean_list = [gx_mean, gy_mean, gz_mean]\n",
    "        g_mean_list.sort() # Sorting list of numbers in ascending\n",
    "        Gm = g_mean_list[2] \n",
    "        Hm = g_mean_list[2]/g_mean_list[1] \n",
    "        Im = g_mean_list[2]/g_mean_list[0] \n",
    "\n",
    "        gx_range = gyro_data[np.argmax(gyro_data[:,0]), 0] - gyro_data[np.argmin(gyro_data[:,0]), 0]\n",
    "        gy_range = gyro_data[np.argmax(gyro_data[:,1]), 1] - gyro_data[np.argmin(gyro_data[:,1]), 1]\n",
    "        gz_range = gyro_data[np.argmax(gyro_data[:,2]), 2] - gyro_data[np.argmin(gyro_data[:,2]), 2]\n",
    "        g_range_list = [gx_range, gy_range, gz_range]\n",
    "        g_range_list.sort() # Sorting list of numbers in ascending\n",
    "        G = g_range_list[2] \n",
    "        H = g_range_list[2]/g_range_list[1] \n",
    "        I = g_range_list[2]/g_range_list[0]\n",
    "\n",
    "\n",
    "        # ----ACCELERATOR FREQUENCY DOMAIN----\n",
    "        freq, Pxx = welch(acc_magnitude, fs=sampling_rate) # use of the fast Fourier transform for the estimation of power spectra\n",
    "        freq_band = np.logical_and(freq >= 0.3, freq <= 15) \n",
    "        power_in_band = Pxx[freq_band] \n",
    "        freq_in_band = freq[freq_band] \n",
    "        #plt.plot(freq,Pxx)\n",
    "\n",
    "        # D and F reflects impact of strides on acceleration\n",
    "        D = np.max(power_in_band) # Feature D: the maximum energy captured by the accelerator, \n",
    "        total_power = np.sum(power_in_band) # Feature F: total power in the frequencies between 0.3 and 15 Hz:\n",
    "\n",
    "        norm_Pxx = Pxx / total_power # normalize the power spectrum\n",
    "        E = entropy(norm_Pxx) # Feature E: normalized information entropy of the discrete FFT component magnitudes\n",
    "\n",
    "        \n",
    "\n",
    "        sorted_idx = np.argsort(power_in_band)[::-1] \n",
    "        first_freq = freq_in_band[sorted_idx[0]] \n",
    "        second_freq = freq_in_band[sorted_idx[1]] \n",
    "        first_power = power_in_band[sorted_idx[0]] \n",
    "        second_power = power_in_band[sorted_idx[1]]\n",
    "\n",
    "        R1 = np.sum(power_in_band[freq_in_band  < 3]) / total_power\n",
    "        R3 = np.sum(Pxx[(freq >= 1.5) & (freq <= 2.5)]) / total_power \n",
    "\n",
    "        \n",
    "\n",
    "        # ----MOVING VS: STANDING----\n",
    "        moving = False\n",
    "        # Append the features to the list\n",
    "        if acc_mag_mean > 0.07:\n",
    "            moving = True\n",
    "        \n",
    "        # Calculate the timestamp for the current window as the median of the timestamps (not necessary for location)\n",
    "        # timestamp = np.median(trace.timestamp[i*window_samples:(i+1)*window_samples]) \n",
    "        # try:\n",
    "        #     # Determine the location label for the current window based on the timestamp\n",
    "        #     loc_label = trace.labels.get('board_loc')\n",
    "        #     # Append the label to the labels list\n",
    "        #     loc_labels.append(loc_label)\n",
    "        # except Exception as error:\n",
    "        #     #print(\"!-This might be testing trace and does not have labels. Error: \",error)\n",
    "        #     pass\n",
    "        features.append([moving, A, B, C, Am,Bm,Cm, acc_mag_mean , \n",
    "                         D, E, total_power, first_freq, first_power,\n",
    "                         G,H,I,Gm,Hm,Im,acc_mag_std])\n",
    "\n",
    "    # Create a DataFrame with the extracted features and location labels\n",
    "    features_df = pd.DataFrame(features, columns=['moving','A', 'B', 'C','Am', 'Bm', 'Cm','acc_mag' ,\n",
    "                                                  'D', 'E', 'total_power', 'first_freq', 'first_power',\n",
    "                                                  'G','H','I','Gm','Hm','Im','acc_std'])\n",
    "    # features_df['loc_label'] = loc_labels\n",
    "    #print(features_df)\n",
    "\n",
    "    return features_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function aims to find peak locations and corresponding values in the signal with the function signal.find_peaks\n",
    "def get_peaks(input_signal, prominence):\n",
    "    peak_locations, _ = signal.find_peaks(input_signal, prominence=prominence)\n",
    "    peak_values = input_signal[peak_locations]\n",
    "    return peak_locations, peak_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resample data to size: fixed length\n",
    "def resample_data(data, fixed_length):\n",
    "    \n",
    "    \n",
    "    num_data_points = len(data)\n",
    "    x_old = np.linspace(0, 1, num_data_points)\n",
    "    x_new = np.linspace(0, 1, fixed_length)\n",
    "    \n",
    "    # 1D interpolation for the magnitude data\n",
    "    interpolator = interp1d(x_old, data, axis=0, kind='linear')\n",
    "    resampled_data = interpolator(x_new)\n",
    "    return resampled_data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the CNN model for magnitute data\n",
    "class CNN1D(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(CNN1D, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(1, 32, kernel_size=5)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.maxpool1 = nn.MaxPool1d(kernel_size=2)\n",
    "        self.conv2 = nn.Conv1d(32, 64, kernel_size=5)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.maxpool2 = nn.MaxPool1d(kernel_size=2)\n",
    "        self.dropout = nn.Dropout(p=0.5)  # Add dropout layer with 50% dropout probability\n",
    "        self.fc1 = nn.Linear(64 * 247, 255)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(255, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.maxpool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.maxpool2(x)\n",
    "        x = self.dropout(x)  # Apply dropout\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu3(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Location------------------------------------------\n",
    "# location_model = joblib.load('./trained_models/location_svm_model.joblib')\n",
    "location_xgboost_model = joblib.load('./trained_models/location_xgboost_model_feature_25.joblib')\n",
    "# activity------------------------------------------\n",
    "activity_xgb_model = joblib.load('trained_models/activity_xgboost_model_feature_3.joblib')\n",
    "# path---------------------------------------------\n",
    "path_cnn_model = torch.load('trained_models/path_index_cnn_model.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_segment_overlap(data,sampling_rate, std_win):\n",
    "    # Calculate raw magnitude of accelerometer signal\n",
    "    # amagn_acc = [np.sqrt(a**2+trace.data['ay'].values[i]**2+trace.data['az'].values[i]**2)for i, a in enumerate(trace.data['ay'].values)]\n",
    "    # Pre-process data\n",
    "    data_seg = pre_process(data)\n",
    "    # Calculate window size\n",
    "    window_size = round(std_win*sampling_rate)\n",
    "    segment_trace = [data_seg[s:s+window_size] for s in range(0, len(data_seg)-window_size, round(window_size/2))]\n",
    "    return segment_trace\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_segment_nonoverlap(data,sampling_rate, std_win):\n",
    "    # Calculate window size\n",
    "    window_size = round(std_win*sampling_rate)\n",
    "    segment_trace = [data[s:s+window_size] for s in range(0, len(data)-window_size, window_size)]\n",
    "    return segment_trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_moving(magn_data):\n",
    "    acc_mag_mean = np.mean(abs(magn_data))\n",
    "    moving = False\n",
    "    # Append the features to the list\n",
    "    if acc_mag_mean > 0.027:\n",
    "        moving = True\n",
    "    return moving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(data):\n",
    "    min_val = min(data)\n",
    "    max_val = max(data)\n",
    "    normalized_data = [(x - min_val) / (max_val - min_val)  - 0.5 for x in data]\n",
    "    return normalized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_watch_loc(trace):\n",
    "    # # 0 accuracy for testing\n",
    "    # y_final = 3\n",
    "\n",
    "    # # XGBoost zyf\n",
    "    # # Get features of data\n",
    "    # features_df = preprocess_and_extract_features(trace)\n",
    "    # features_df = features_df[features_df['moving']] # Keep only moving windows\n",
    "    # X_trace = features_df.drop(['Gm', 'Hm', 'Im'], axis=1)\n",
    "    # if np.shape(X_trace)[0] != 0:\n",
    "\n",
    "    #     # Prepare data for classification\n",
    "    #     scaler = StandardScaler()\n",
    "    #     X_test = scaler.fit_transform(X_trace)\n",
    "\n",
    "    #     # Create the XGBoost DMatrix object for the test data\n",
    "    #     dtest = xgb.DMatrix(X_test)\n",
    "\n",
    "    #     # Make predictions on the test set and evaluate the model\n",
    "    #     y_pred = location_xgboost_model.predict(dtest)\n",
    "    #     # y_pred = np.squeeze(y_pred)\n",
    "    #     y_final = np.argmax(np.bincount(y_pred.astype(int)))\n",
    "\n",
    "    # XGboost szc\n",
    "    # get segment data\n",
    "    amagn = [np.sqrt(a**2+trace.data['ay'].values[i]**2+trace.data['az'].values[i]**2)for i, a in enumerate(trace.data['ax'].values)]\n",
    "    std_win = 3 # length of window in seconds\n",
    "    sampling_rate = 200\n",
    "    segment_trace = get_segment_overlap(amagn, sampling_rate, std_win)\n",
    "    # add feature extraction\n",
    "    num_features = 25\n",
    "    featured_trace = np.zeros((np.shape(segment_trace)[0], num_features))\n",
    "    for i in range(np.shape(segment_trace)[0]):\n",
    "        featured_trace[i,] = get_features_25(segment_trace[i])\n",
    "    # Create the XGBoost DMatrix object for the test data\n",
    "    dtest = xgb.DMatrix(featured_trace)\n",
    "\n",
    "    # Make predictions on the test set and evaluate the model\n",
    "    y_pred = location_xgboost_model.predict(dtest)\n",
    "    y_final = np.argmax(np.bincount(y_pred.astype(int)))\n",
    "    \n",
    "    return y_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_path_idx(trace):\n",
    "    # # 0 accuracy for testing\n",
    "    # path_idx = 5\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    magn_mag = [np.sqrt(m**2+trace.data['my'].values[i]**2+trace.data['mz'].values[i]**2)for i, m in enumerate(trace.data['mx'].values)]\n",
    "    X_trace = torch.tensor(np.expand_dims(resample_data(magn_mag, 1000), axis=(0, -1)), dtype=torch.float32)\n",
    "    X_trace = X_trace.permute(0, 2, 1).to(device)\n",
    "    \n",
    "    path_cnn_model.to(device)\n",
    "    path_cnn_model.eval()\n",
    "    path_idx = int(np.argmax(path_cnn_model(X_trace).cpu().detach().numpy()))\n",
    "    return path_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_step_count(trace):\n",
    "    # # 0 accuracy for testing\n",
    "    # stepCount = 0\n",
    "\n",
    "    # Calculate raw magnitude of accelerometer signal\n",
    "    amagn = [np.sqrt(a**2+trace.data['ay'].values[i]**2+trace.data['az'].values[i]**2)for i, a in enumerate(trace.data['ax'].values)]\n",
    "    # Filter the signal to get more accurate results -----------------------------------------------------------\n",
    "  \n",
    "    data_filtered = pre_process(amagn)\n",
    "    # Use convolution to reduce noise in signal again\n",
    "    filter_window_size = 40\n",
    "    data_filtered = np.convolve(data_filtered, np.ones((filter_window_size,))/filter_window_size, mode='valid')\n",
    "    # Find peaks in the filtered signal and realize our stepcount -----------------------------------------------\n",
    "    # TODO: modify prominence\n",
    "    # Segment data into windows --------------------------------------------------------------------------------\n",
    "    std_win = 1 # length of window in seconds\n",
    "    sampling_rate = 200\n",
    "    data_segmented = get_segment_nonoverlap(data_filtered, sampling_rate, std_win)\n",
    "    # Normalize data in each windows ---------------------------------------------------------------------------\n",
    "    win_size = round(std_win * sampling_rate)\n",
    "    normalized_data = data_filtered.copy()\n",
    "    for i, seg in enumerate(data_segmented):\n",
    "        if check_moving(seg):\n",
    "            normalized_data[i*win_size:i*win_size + win_size] = normalize(seg)\n",
    "        else:\n",
    "            normalized_data[i*win_size:i*win_size + win_size] = 0\n",
    "    # check data after last window\n",
    "    if np.shape(data_segmented)[0]*win_size < len(data_filtered):\n",
    "        seg = data_filtered[np.shape(data_segmented)[0]*win_size:]\n",
    "        if check_moving(seg):\n",
    "                normalized_data[i*win_size:i*win_size + len(seg)] = normalize(seg)\n",
    "        else:\n",
    "            normalized_data[i*win_size:i*win_size + len(seg)] = 0\n",
    "\n",
    "    # Find peaks in the filtered signal and realize our stepcount -----------------------------------------------\n",
    "    prominence = 0.4\n",
    "    peak_locations, _ = get_peaks(normalized_data, prominence)\n",
    "    stepCount = len(peak_locations)\n",
    "\n",
    "    # # plot normalized data\n",
    "    # fig, axes = plt.subplots(2,1, figsize=(60, 5)) #figsize is width, height\n",
    "    # # axes[0].set_title(title)\n",
    "    # axes[0].plot(data_filtered, alpha=1, label=\"Filtered mag\")\n",
    "    # axes[1].plot(normalized_data, alpha=1, label=\"Normalaized mag\")\n",
    "    # # colors = ['r', 'g']\n",
    "    # # for s in  range(0, len(acc_magnitude), win_size): # adding vertical lines for the peak detection windows\n",
    "    # #     axes[1].axvline(s, color = 'r')\n",
    "    # axes[1].plot(peak_locations, normalized_data[peak_locations], 'y+', color=\"red\", label=\"Peak Locations\")\n",
    "    \n",
    "    return stepCount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_activity(trace):\n",
    "    # # 0 accuracy for testing\n",
    "    # stand, walk, run, cycle = 2, 2, 2, 2\n",
    "\n",
    "    # algorithm here\n",
    "    stand, walk, run, cycle = 0, 0, 0, 0\n",
    "    \n",
    "    # get segment data\n",
    "    amagn = [np.sqrt(a**2+trace.data['ay'].values[i]**2+trace.data['az'].values[i]**2)for i, a in enumerate(trace.data['ax'].values)]\n",
    "    std_win = 3 # length of window in seconds\n",
    "    sampling_rate = 200\n",
    "    segment_trace = get_segment_overlap(amagn, sampling_rate, std_win)\n",
    "    # add feature extraction\n",
    "    num_features = 3\n",
    "    featured_trace = np.zeros((np.shape(segment_trace)[0], num_features))\n",
    "    for i in range(np.shape(segment_trace)[0]):\n",
    "        featured_trace[i,] = get_features_3(segment_trace[i])\n",
    "    # Create the XGBoost DMatrix object for the test data\n",
    "    dtest = xgb.DMatrix(featured_trace)\n",
    "\n",
    "    # Make predictions on the test set and evaluate the model\n",
    "    y_pred = activity_xgb_model.predict(dtest)\n",
    "    # filter prediction by 60s\n",
    "    # Sliding window: 60s\n",
    "    std_win = 10\n",
    "    n = round (60 / std_win * 2 - 1)\n",
    "    y_pred_count = np.zeros(4)\n",
    "    \n",
    "    y_pred_60 = y_pred.copy()\n",
    "    for s in range(0, len(y_pred) - n, int(n/2+1)):\n",
    "        # window = 60s \n",
    "        windowed_label = y_pred[s : s+n]\n",
    "        for j in range(n): \n",
    "            # Find the label that appears the most\n",
    "            for k in range(4):\n",
    "                if windowed_label[j] == k:\n",
    "                    y_pred_count[k]+=1\n",
    "        label_argmax = np.where(y_pred_count == np.max(y_pred_count))\n",
    "        # print(label_argmax)\n",
    "        if len(label_argmax)==1:\n",
    "            y_pred_60[s : s+n] = np.argmax(y_pred_count)\n",
    "    \n",
    "    # remove duplicated elements\n",
    "    predicted = list(set(y_pred_60))\n",
    "    if 0 in predicted:\n",
    "        stand = 1\n",
    "    if 1 in predicted:\n",
    "        walk = 1\n",
    "    if 2 in predicted:\n",
    "        run = 1\n",
    "    if 3 in predicted:\n",
    "        cycle = 1    \n",
    "    return stand, walk, run, cycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [32, 3, 5], expected input[1, 1, 1000] to have 3 channels, but got 1 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_20064\\4238892667.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[0mcategorization_results\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'watch_loc'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_watch_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m     \u001b[0mcategorization_results\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'path_idx'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_path_idx\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m     \u001b[0mcategorization_results\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'step_count'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_step_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[0mcategorization_results\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'stand'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcategorization_results\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'walk'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcategorization_results\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'run'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcategorization_results\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'cycle'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_activity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_20064\\212830262.py\u001b[0m in \u001b[0;36mget_path_idx\u001b[1;34m(trace)\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mpath_cnn_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mpath_cnn_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[0mpath_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_cnn_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_trace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mpath_idx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\szzcc\\.conda\\envs\\mhealth23\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1195\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_20064\\805065892.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmaxpool1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\szzcc\\.conda\\envs\\mhealth23\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1195\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\szzcc\\.conda\\envs\\mhealth23\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    311\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    312\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 313\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    314\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    315\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\szzcc\\.conda\\envs\\mhealth23\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    308\u001b[0m                             _single(0), self.dilation, self.groups)\n\u001b[0;32m    309\u001b[0m         return F.conv1d(input, weight, bias, self.stride,\n\u001b[1;32m--> 310\u001b[1;33m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[0;32m    311\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    312\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Given groups=1, weight of size [32, 3, 5], expected input[1, 1, 1000] to have 3 channels, but got 1 channels instead"
     ]
    }
   ],
   "source": [
    "# Loop through all traces and calculate the step count for each trace\n",
    "solution_file = []\n",
    "for idx, filename in enumerate(filenames):\n",
    "    # print(filename)\n",
    "    trace = Recording(filename, no_labels=True, mute=True)\n",
    "    categorization_results = {'watch_loc': 0, 'path_idx': 0, 'step_count': 0, 'stand': 0, 'walk': 0, 'run': 0, 'cycle': 0}\n",
    "\n",
    "    #\n",
    "    # Your algorithm goes here\n",
    "    # Make sure, you do not use the gps data and are tolerant for missing data (see task set).\n",
    "    # Your program must not crash when single smartphone data traces are missing.\n",
    "    #\n",
    "\n",
    "    categorization_results['watch_loc'] = get_watch_loc(trace)\n",
    "    categorization_results['path_idx'] = get_path_idx(trace)\n",
    "    categorization_results['step_count'] = get_step_count(trace)\n",
    "    categorization_results['stand'], categorization_results['walk'], categorization_results['run'], categorization_results['cycle'] = get_activity(trace)\n",
    "\n",
    "\n",
    "    # Append your calculated results and the id of each trace and category to the solution file\n",
    "    trace_id = ''.join([*filename][-8:-5])\n",
    "    for counter_label, category in enumerate(categorization_results):\n",
    "        solution_file.append([trace_id + f'_{counter_label+1}', categorization_results[category]])\n",
    "    # show progress\n",
    "    if (idx+1)%10 == 0:\n",
    "        print(\"Process traces: \", idx+1, '/', len(filenames))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Write the detected step counts into a .csv file to then upload the .csv file to Kaggle\n",
    "# When cross-checking the .csv file on your computer, we recommend using the text editor and NOT excel so that the results are displayed correctly\n",
    "# IMPORTANT: Do NOT change the name of the columns ('Id' and 'Category') of the .csv file\n",
    "submission_file_df = pd.DataFrame(np.asarray(solution_file), columns=['Id', 'Category'])\n",
    "submission_file_df.to_csv('results/submission_12.csv', header=['Id', 'Category'], index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
