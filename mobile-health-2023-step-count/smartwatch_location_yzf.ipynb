{"cells":[{"cell_type":"code","execution_count":48,"metadata":{"execution":{"iopub.execute_input":"2023-03-21T17:50:11.957682Z","iopub.status.busy":"2023-03-21T17:50:11.956849Z","iopub.status.idle":"2023-03-21T17:50:11.962109Z","shell.execute_reply":"2023-03-21T17:50:11.960868Z","shell.execute_reply.started":"2023-03-21T17:50:11.957616Z"},"pycharm":{"name":"#%%\n"},"trusted":true},"outputs":[],"source":["# This is the template for the submission. If you want, you can develop your algorithm in a regular Python script and copy the code here for submission.\n","\n","# Team members (e-mail, legi):\n","# zhisun@ethz.ch, 22-958-227\n","# enjcao@ethz.ch, 22-942-700\n","# yifzhou@ethz.ch, 22-940-381"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["%pip install xgboost "]},{"cell_type":"code","execution_count":5,"metadata":{"collapsed":false,"execution":{"iopub.execute_input":"2023-03-21T17:50:12.356641Z","iopub.status.busy":"2023-03-21T17:50:12.355451Z","iopub.status.idle":"2023-03-21T17:50:12.363393Z","shell.execute_reply":"2023-03-21T17:50:12.362041Z","shell.execute_reply.started":"2023-03-21T17:50:12.356594Z"},"jupyter":{"outputs_hidden":false},"pycharm":{"name":"#%%\n"},"trusted":true},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import os\n","\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.svm import SVC\n","from sklearn.model_selection import train_test_split, GridSearchCV\n","from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n","from scipy.stats import entropy\n","from scipy.signal import welch\n","from scipy.fftpack import fft\n","\n","from Lilygo.Recording import Recording, data_integrity\n","from Lilygo.Dataset import Dataset\n","\n","import joblib\n","import math\n","\n","import xgboost as xgb\n","from xgboost import XGBClassifier"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Filtering and Feature Extraction"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["# data pre-processing\n","# This function aims to find the component caused by gravity from data, which means the signal around 0 Hz\n","def get_gravity(data):\n","    filtered_data = np.zeros_like(data)\n","    # Parameters in IIR filter\n","    alpla = [1, -1.979133761292768, 0.979521463540373]\n","    beta = [0.000086384997973502, 0.00012769995947004, 0.000086384997973502]\n","    # Formula of IIR filter\n","    for i in range(2, len(data)):\n","        filtered_data[i] = alpla[0] * (data[i] * beta[0] + data[i-1] * beta[1] + data[i-2] * beta[2] - filtered_data[i-1] * alpla[1] - filtered_data[i-2] * alpla[2])\n","    return filtered_data\n","\n","def get_highpass(data):\n","    filtered_data = np.zeros_like(data)  # filtered_data\n","    alpla = [1, -1.905384612118461, 0.910092542787947]\n","    beta = [0.953986986993339, -1.907503180919730, 0.953986986993339]\n","\n","    for i in range(2, len(data)):\n","        filtered_data[i] = alpla[0] * (data[i] * beta[0] + data[i-1] * beta[1] + data[i-2] * beta[2] - filtered_data[i-1] * alpla[1] - filtered_data[i-2] * alpla[2])\n","    return filtered_data\n","\n","# This funciton aims to realize a high-pass filter with cutoff frequency = 5 Hz. Because according to massive amounts of data, the general \n","# maximum frequency of human walking is about 5 Hz\n","def get_lowpass(data):\n","    filtered_data = np.zeros_like(data)  # filtered_data\n","    alpla = [1, -1.80898117793047, 0.827224480562408]\n","    beta = [0.096665967120306, -0.172688631608676, 0.095465967120306]\n","\n","    for i in range(2, len(data)):\n","        filtered_data[i] = alpla[0] * (data[i] * beta[0] + data[i-1] * beta[1] + data[i-2] * beta[2] - filtered_data[i-1] * alpla[1] - filtered_data[i-2] * alpla[2])\n","    return filtered_data\n","\n","def pre_process(data):\n","    # Find the component caused by gravity from data and remove it from the singanl\n","    data_gravity = get_gravity(data)\n","    data_user = data - data_gravity\n","    # Get user's acceleration along the gravity direction by dot product\n","    data_acc = data_user * data_gravity\n","    # Add low pass and high pass filter to reduce noise in signal (possible human walking rate:1 - 5Hz)\n","    data_filtered = get_highpass(data_acc)\n","    data_filtered = get_lowpass(data_filtered)\n","    return data_filtered\n","\n","def preprocess_and_extract_features(trace, window_size=15, sampling_rate=200):\n","    \"\"\"\n","    Preprocess the data and extract features from the 3D accelerometer, gyroscope, and magnetometer data.\n","\n","    Args:\n","    trace (Lilygo.Recording.Recording): Object containing the raw data with accelerometer data stored in lists (e.g. trace.data['ax'])\n","    window_size (int): The window size in seconds for splitting the data\n","    sampling_rate (int): The sampling rate of the data in Hz\n","\n","    Returns:\n","    pd.DataFrame: A DataFrame with the extracted features and location labels\n","    \"\"\"\n","    # Read data from trace\n","    ax = pre_process(trace.data['ax'].values)\n","    ay = pre_process(trace.data['ay'].values)\n","    az = pre_process(trace.data['az'].values)\n","    \n","    gx = pre_process(trace.data['gx'].values)\n","    gy = pre_process(trace.data['gy'].values)\n","    gz = pre_process(trace.data['gz'].values)\n","    \n","    '''mx = trace.data['mx'].values\n","    my = trace.data['my'].values\n","    mz = trace.data['mz'].values'''\n","    \n","    # Compute the length of each window in samples\n","    window_samples = window_size * sampling_rate\n","    \n","    # Compute the number of windows in the recording\n","    num_windows = len(ax) // window_samples\n","\n","    # Initialize lists for storing extracted features and location labels\n","    features = []\n","    loc_labels = []\n","\n","    # Helper function to compute the magnitude of a vector\n","    magnitude = lambda vec: np.sqrt(np.sum(vec**2, axis=1))\n","\n","    for i in range(num_windows):\n","       \n","        # Extract the accelerometer, gyroscope, and magnetometer data for the current window\n","        acc_data = np.array([ax[i*window_samples:(i+1)*window_samples],\n","                             ay[i*window_samples:(i+1)*window_samples],\n","                             az[i*window_samples:(i+1)*window_samples]]).T\n","        gyro_data = np.array([gx[i*window_samples:(i+1)*window_samples],\n","                              gy[i*window_samples:(i+1)*window_samples],\n","                              gz[i*window_samples:(i+1)*window_samples]]).T\n","        '''mag_data = np.array([mx[i*window_samples:(i+1)*window_samples],\n","                             my[i*window_samples:(i+1)*window_samples],\n","                             mz[i*window_samples:(i+1)*window_samples]]).T'''\n","        \n","        '''figure,ax = plt.subplots(3, 1, figsize=(10, 6))\n","        ax[0].plot(acc_data[:,0])\n","        ax[0].set_ylabel('ax')\n","        ax[1].plot(acc_data[:,1])\n","        ax[1].set_ylabel('ay')\n","        ax[2].plot(acc_data[:,2])\n","        ax[2].set_ylabel('az')'''\n","\n","        # Compute magnitudes\n","        acc_magnitude = np.sqrt(np.sum(acc_data**2, axis=1))\n","        acc_mag_mean = abs(np.mean(acc_magnitude))\n","        acc_mag_std = np.std(acc_magnitude)\n","        #gyro_magnitude = magnitude(gyro_data)\n","        #mag_magnitude = magnitude(mag_data)\n","\n","\n","        # ----ACCELERATOR TIME DOMAIN----\n","        ax_mean = abs(np.mean(acc_data[:,0]))\n","        ay_mean = abs(np.mean(acc_data[:,1]))\n","        az_mean = abs(np.mean(acc_data[:,2]))\n","        a_mean_list = [ax_mean, ay_mean, az_mean]\n","        a_mean_list.sort() # Sorting list of numbers in ascending\n","        Am = a_mean_list[2] # Feature Am: the maximum mean among all dimensions (represents motion range for location)\n","        Bm = a_mean_list[2]/a_mean_list[1] # Feature Bm and Cm: ratio of the maximum mean in different axes (represents DoF in movement for location)\n","        Cm = a_mean_list[2]/a_mean_list[0] \n","\n","        ax_range = acc_data[np.argmax(acc_data[:,0]), 0] - acc_data[np.argmin(acc_data[:,0]), 0]\n","        ay_range = acc_data[np.argmax(acc_data[:,1]), 1] - acc_data[np.argmin(acc_data[:,1]), 1]\n","        az_range = acc_data[np.argmax(acc_data[:,2]), 2] - acc_data[np.argmin(acc_data[:,2]), 2]\n","        a_range_list = [ax_range, ay_range, az_range]\n","        a_range_list.sort() # Sorting list of numbers in ascending\n","        A = a_range_list[2] # Feature A: the maximum range among all dimensions (represents motion range for location)\n","        B = a_range_list[2]/a_range_list[1] # Feature B and C: ratio of the maximum ranges in different axes (represents DoF in movement for location)\n","        C = a_range_list[2]/a_range_list[0] \n","        \n","        \n","        # ----GYROSCOPE TIME DOMAIN----\n","        gx_mean = abs(np.mean(gyro_data[:,0]))\n","        gy_mean = abs(np.mean(gyro_data[:,1]))\n","        gz_mean = abs(np.mean(gyro_data[:,2]))\n","        g_mean_list = [gx_mean, gy_mean, gz_mean]\n","        g_mean_list.sort() # Sorting list of numbers in ascending\n","        Gm = g_mean_list[2] \n","        Hm = g_mean_list[2]/g_mean_list[1] \n","        Im = g_mean_list[2]/g_mean_list[0] \n","\n","        gx_range = gyro_data[np.argmax(gyro_data[:,0]), 0] - gyro_data[np.argmin(gyro_data[:,0]), 0]\n","        gy_range = gyro_data[np.argmax(gyro_data[:,1]), 1] - gyro_data[np.argmin(gyro_data[:,1]), 1]\n","        gz_range = gyro_data[np.argmax(gyro_data[:,2]), 2] - gyro_data[np.argmin(gyro_data[:,2]), 2]\n","        g_range_list = [gx_range, gy_range, gz_range]\n","        g_range_list.sort() # Sorting list of numbers in ascending\n","        G = g_range_list[2] \n","        H = g_range_list[2]/g_range_list[1] \n","        I = g_range_list[2]/g_range_list[0]\n","\n","\n","        # ----ACCELERATOR FREQUENCY DOMAIN----\n","        freq, Pxx = welch(acc_magnitude, fs=sampling_rate) # use of the fast Fourier transform for the estimation of power spectra\n","        freq_band = np.logical_and(freq >= 0.3, freq <= 15) \n","        power_in_band = Pxx[freq_band] \n","        freq_in_band = freq[freq_band] \n","        #plt.plot(freq,Pxx)\n","\n","        # D and F reflects impact of strides on acceleration\n","        D = np.max(power_in_band) # Feature D: the maximum energy captured by the accelerator, \n","        total_power = np.sum(power_in_band) # Feature F: total power in the frequencies between 0.3 and 15 Hz:\n","\n","        norm_Pxx = Pxx / total_power # normalize the power spectrum\n","        E = entropy(norm_Pxx) # Feature E: normalized information entropy of the discrete FFT component magnitudes\n","\n","        \n","\n","        sorted_idx = np.argsort(power_in_band)[::-1] \n","        first_freq = freq_in_band[sorted_idx[0]] \n","        second_freq = freq_in_band[sorted_idx[1]] \n","        first_power = power_in_band[sorted_idx[0]] \n","        second_power = power_in_band[sorted_idx[1]]\n","\n","        R1 = np.sum(power_in_band[freq_in_band  < 3]) / total_power\n","        R3 = np.sum(Pxx[(freq >= 1.5) & (freq <= 2.5)]) / total_power \n","\n","        \n","\n","        # ----MOVING VS: STANDING----\n","        moving = False\n","        # Append the features to the list\n","        if acc_mag_mean > 0.1 and total_power >0.0001:\n","            moving = True\n","        \n","        # Calculate the timestamp for the current window as the median of the timestamps (not necessary for location)\n","        # timestamp = np.median(trace.timestamp[i*window_samples:(i+1)*window_samples]) \n","        try:\n","            # Determine the location label for the current window based on the timestamp\n","            loc_label = trace.labels.get('board_loc')\n","            # Append the label to the labels list\n","            loc_labels.append(loc_label)\n","        except Exception as error:\n","            #print(\"!-This might be testing trace and does not have labels. Error: \",error)\n","            pass\n","        features.append([moving, A, B, C, Am,Bm,Cm, acc_mag_mean , \n","                         D, E, total_power, first_freq, first_power,\n","                         G,H,I,Gm,Hm,Im,acc_mag_std])\n","\n","    # Create a DataFrame with the extracted features and location labels\n","    features_df = pd.DataFrame(features, columns=['moving','A', 'B', 'C','Am', 'Bm', 'Cm','acc_mag' ,\n","                                                  'D', 'E', 'total_power', 'first_freq', 'first_power',\n","                                                  'G','H','I','Gm','Hm','Im','acc_std'])\n","    features_df['loc_label'] = loc_labels\n","    #print(features_df)\n","\n","    return features_df"]},{"cell_type":"code","execution_count":53,"metadata":{},"outputs":[{"data":{"text/plain":["\"# Get features of training traces\\nX = []\\ny = []\\nfor traceName in traceNames[3:20]:\\n    if traceName[-5:] == '.json':\\n        trace = Recording(path_training+'/'+ traceName, no_labels=False, mute=True)\\n\\n        figure, ax = plt.subplots(6,1,figsize=(60, 5))\\n        figure.suptitle(str(trace.labels))\\n        ax[0].plot(get_lowpass(get_highpass(trace.data['ax'].values)))\\n        ax[0].plot(get_lowpass(get_highpass(trace.data['ay'].values)))\\n        ax[0].plot(get_lowpass(get_highpass(trace.data['az'].values)))\\n        \\n        \\n        # Get features of data\\n        features_df = preprocess_and_extract_features(trace)\\n        ax[1].plot(features_df['acc_mag'])\\n        ax[1].fill_between(np.arange(len(features_df['acc_mag'])), 0, \\n                           features_df['acc_mag'], \\n                           where=features_df['moving']==True, \\n                           color='red', alpha=0.3)\\n\\n        ax[2].plot(features_df['A'])\\n        ax[2].plot(features_df['B'])\\n        ax[2].plot(features_df['C'])\\n        ax[2].legend(['A: Acc-Max_mag','B: Ratio','C: Ratio'])\\n\\n        ax[3].plot(features_df['D'])\\n        #ax[3].plot(features_df['E']) # Entropy makes no difference for location\\n        ax[3].plot(features_df['F'])\\n        ax[3].legend(['D: Max_energy','F: Overall energy'])\\n\\n        ax[4].plot(features_df['G'])\\n        ax[4].plot(features_df['H'])\\n        ax[4].plot(features_df['I'])\\n        ax[4].legend(['G: Gyro-Max_mag','B: Ratio','C: Ratio'])\\n\\n        ax[5].plot(features_df['acc_std'])\""]},"execution_count":53,"metadata":{},"output_type":"execute_result"}],"source":["# path_training = '/kaggle/input/mobile-health-2023-path-detection/data/train'\n","path_training = \"./data/train/\"\n","traceNames=os.listdir(path_training)\n","\n","'''# Get features of training traces\n","X = []\n","y = []\n","for traceName in traceNames[3:20]:\n","    if traceName[-5:] == '.json':\n","        trace = Recording(path_training+'/'+ traceName, no_labels=False, mute=True)\n","\n","        figure, ax = plt.subplots(6,1,figsize=(60, 5))\n","        figure.suptitle(str(trace.labels))\n","        ax[0].plot(get_lowpass(get_highpass(trace.data['ax'].values)))\n","        ax[0].plot(get_lowpass(get_highpass(trace.data['ay'].values)))\n","        ax[0].plot(get_lowpass(get_highpass(trace.data['az'].values)))\n","        \n","        \n","        # Get features of data\n","        features_df = preprocess_and_extract_features(trace)\n","        ax[1].plot(features_df['acc_mag'])\n","        ax[1].fill_between(np.arange(len(features_df['acc_mag'])), 0, \n","                           features_df['acc_mag'], \n","                           where=features_df['moving']==True, \n","                           color='red', alpha=0.3)\n","\n","        ax[2].plot(features_df['A'])\n","        ax[2].plot(features_df['B'])\n","        ax[2].plot(features_df['C'])\n","        ax[2].legend(['A: Acc-Max_mag','B: Ratio','C: Ratio'])\n","\n","        ax[3].plot(features_df['D'])\n","        #ax[3].plot(features_df['E']) # Entropy makes no difference for location\n","        ax[3].plot(features_df['F'])\n","        ax[3].legend(['D: Max_energy','F: Overall energy'])\n","\n","        ax[4].plot(features_df['G'])\n","        ax[4].plot(features_df['H'])\n","        ax[4].plot(features_df['I'])\n","        ax[4].legend(['G: Gyro-Max_mag','B: Ratio','C: Ratio'])\n","\n","        ax[5].plot(features_df['acc_std'])'''"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Load training traces and train"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"data":{"text/plain":["'if len(set(y_train)) < 2:\\n    raise ValueError(\"The number of classes has to be greater than one; got %d class\" % len(set(y_train)))'"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["# path_training = '/kaggle/input/mobile-health-2023-path-detection/data/train'\n","# path_training = \"./data/train/\"\n","dir_data = 'E:\\\\Sunzhichao\\\\ETHz\\\\2223Spring\\\\Mobile_Health\\\\data\\\\'\n","path_training = dir_data + 'train\\\\'\n","traceNames=os.listdir(path_training)\n","\n","# Get features of training traces\n","X = pd.DataFrame()\n","y = pd.DataFrame()\n","for traceName in traceNames[0:220]:\n","    if traceName[-5:] == '.json':\n","        trace = Recording(path_training+'/'+ traceName, no_labels=False, mute=True)\n","\n","        features_df = preprocess_and_extract_features(trace)\n","        features_df = features_df[features_df['moving']] # Keep only moving windows\n","        \n","        # Prepare data for classification\n","        # X_trace = features_df.drop('loc_label', axis=1)\n","        X_trace = features_df.drop(['loc_label', 'Gm', 'Hm', 'Im'], axis=1)\n","        y_trace = features_df['loc_label']\n","        X = pd.concat([X, X_trace], axis=0).reset_index(drop=True)\n","        y = pd.concat([y, y_trace], axis=0).reset_index(drop=True)\n","        \n","# Normalize the features\n","scaler = StandardScaler()\n","X_scaled = scaler.fit_transform(X)\n","\n","# Using features of all training traces as training set\n","X_train = X_scaled\n","y_train = y\n","#X_train, _, y_train, _ = train_test_split(X_scaled, y, test_size=0.01, random_state=42)\n","\n","# Check the number of classes in y_train\n","'''if len(set(y_train)) < 2:\n","    raise ValueError(\"The number of classes has to be greater than one; got %d class\" % len(set(y_train)))'''\n","  "]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Train XGBoost with determined hyperparameters"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[14:51:15] WARNING: C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n","Parameters: { \"n_estimators\" } might not be used.\n","\n","  This could be a false alarm, with some parameters getting used by language bindings but\n","  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n","  but getting flagged wrongly here. Please open an issue if you find any such cases.\n","\n","\n","Training XGBoost classifier for activity\n","accuracy:  0.9548872180451128\n"]},{"data":{"text/plain":["['./trained_models/location_xgboost_model.joblib']"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size = 0.3, shuffle = True)\n","\n","# Create the XGBoost DMatrix object\n","dtrain = xgb.DMatrix(X_train, label=y_train)\n","\n","# Set the parameters for the XGBoost model\n","params = {'eta': 0.3, 'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 1000, 'num_class': 3, 'objective': 'multi:softmax'}\n","\n","# Train the model\n","xgb_model = xgb.train(params, dtrain)\n","print(\"Training XGBoost classifier for activity\")\n","\n","# Create the XGBoost DMatrix object for the test data\n","dtest = xgb.DMatrix(X_test)\n","y_pred = xgb_model.predict(dtest)\n","\n","print(\"accuracy: \", accuracy_score(y_test, y_pred))\n","\n","# Save the trained xgboost model\n","joblib.dump(xgb_model, './trained_models/location_xgboost_model.joblib')\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Train SVM"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Train the SVM classifier with RBF kernel\n","param_grid = {'C': [0.1, 1, 10, 100], 'gamma': [1, 0.1, 0.01, 0.001]}\n","grid = GridSearchCV(SVC(kernel='rbf'), param_grid, refit=True, verbose=2)\n","grid.fit(X_train, np.ravel(y_train))\n","\n","# Print the best hyperparameters and the corresponding accuracy score\n","print(\"Best Hyperparameters: \", grid.best_params_)\n","print(\"Best Accuracy Score: \", grid.best_score_)\n","\n","# Save the trained SVM model\n","joblib.dump(grid.best_estimator_, './trained_models/location_svm_model.joblib')#\n","# joblib.dump(grid, './trained_models/location_svm_model.joblib')"]},{"cell_type":"code","execution_count":55,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Best Hyperparameters:  {'eta': 0.3, 'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 1000, 'num_class': 3, 'objective': 'multi:softmax'}\n","Best Accuracy Score:  0.916336131918875\n"]}],"source":["model = 'xgBoost'\n","\n","if model == 'SVM':\n","    # Train the SVM classifier with RBF kernel\n","    param_grid = {\n","        'C': [0.1, 1, 10, 100], \n","        'gamma': [1, 0.1, 0.01, 0.001]\n","        }\n","    grid = GridSearchCV(SVC(kernel='rbf'), param_grid, refit=True, verbose=2)\n","    grid.fit(X_train, np.ravel(y_train))\n","    \n","    # Save the trained SVM model\n","    joblib.dump(grid.best_estimator_, './trained_models/location_svm_model.joblib')#\n","    # joblib.dump(grid, './trained_models/location_svm_model.joblib')\n","    \n","elif model == 'xgBoost':\n","    # Define the XGBoost DMatrix object\n","    dtrain = xgb.DMatrix(X_train, label=y_train)\n","\n","    # Define the hyperparameters to be tuned\n","    params = {\n","        'max_depth': [3, 5, 7],\n","        'learning_rate': [0.1, 0.05, 0.01],\n","        'eta': [0.3, 0.2, 0.1, 0.05, 0.01],\n","        'n_estimators': [50, 100, 500, 1000],\n","        'objective': ['multi:softmax'],\n","        'num_class': [3], #Best Hyperparameters:  {'eta': 0.3, 'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 1000, 'num_class': 3, 'objective': 'multi:softmax'} Best Accuracy Score:  0.916336131918875\n","    }\n","    '''params = {\n","        'max_depth': [3, 7],\n","        'learning_rate': [0.1, 0.01],\n","        'eta': [ 0.2, 0.1],\n","        'n_estimators': [50, 100],\n","        'objective': ['multi:softmax'],\n","        'num_class': [3],\n","    } ''' #Best Hyperparameters:  {'eta': 0.2, 'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 100, 'num_class': 3, 'objective': 'multi:softmax'} Best Accuracy Score:  0.9072221049713012\n","\n","    # Define the XGBoost classifier\n","    xgb_classifier = xgb.XGBClassifier()\n","\n","    # Define the GridSearchCV object\n","    grid = GridSearchCV(estimator = xgb_classifier, param_grid = params, cv=5, n_jobs=-1, scoring='accuracy')\n","\n","    # Fit the GridSearchCV object to the training data\n","    grid.fit(X_train, y_train)\n","\n","    \n","    # Save the trained xgboost model\n","    joblib.dump(grid, './trained_models/location_xgboost_model.joblib')\n","\n","\n","# Print the best hyperparameters and the corresponding accuracy score\n","print(\"Best Hyperparameters: \", grid.best_params_)\n","print(\"Best Accuracy Score: \", grid.best_score_)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Predict on testing traces with trained model and Evaluation predictions"]},{"cell_type":"code","execution_count":54,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["(figure above, if pred wrong) label: 1 prediction: 0\n","(figure above, if pred wrong) label: 2 prediction: 2\n","(figure above, if pred wrong) label: 1 prediction: 0\n","(figure above, if pred wrong) label: 0 prediction: 2\n","(figure above, if pred wrong) label: 1 prediction: 0\n","(figure above, if pred wrong) label: 0 prediction: 0\n","(figure above, if pred wrong) label: 2 prediction: 2\n","(figure above, if pred wrong) label: 1 prediction: 0\n","(figure above, if pred wrong) label: 0 prediction: 2\n","(figure above, if pred wrong) label: 2 prediction: 0\n","(figure above, if pred wrong) label: 1 prediction: 0\n","(figure above, if pred wrong) label: 2 prediction: 0\n","(figure above, if pred wrong) label: 0 prediction: 0\n","(figure above, if pred wrong) label: 1 prediction: 1\n","(figure above, if pred wrong) label: 2 prediction: 0\n","(figure above, if pred wrong) label: 0 prediction: 0\n","(figure above, if pred wrong) label: 1 prediction: 0\n","(figure above, if pred wrong) label: 0 prediction: 0\n","(figure above, if pred wrong) label: 1 prediction: 0\n","(figure above, if pred wrong) label: 1 prediction: 2\n","(figure above, if pred wrong) label: 2 prediction: 0\n","(figure above, if pred wrong) label: 1 prediction: 0\n","(figure above, if pred wrong) label: 2 prediction: 0\n","(figure above, if pred wrong) label: 0 prediction: 0\n","(figure above, if pred wrong) label: 1 prediction: 2\n","(figure above, if pred wrong) label: 0 prediction: 0\n","(figure above, if pred wrong) label: 0 prediction: 0\n","(figure above, if pred wrong) label: 0 prediction: 0\n","(figure above, if pred wrong) label: 1 prediction: 0\n","(figure above, if pred wrong) label: 1 prediction: 0\n","(figure above, if pred wrong) label: 1 prediction: 2\n","(figure above, if pred wrong) label: 2 prediction: 0\n","(figure above, if pred wrong) label: 2 prediction: 0\n","(figure above, if pred wrong) label: 1 prediction: 2\n","(figure above, if pred wrong) label: 1 prediction: 2\n","(figure above, if pred wrong) label: 1 prediction: 0\n","(figure above, if pred wrong) label: 2 prediction: 0\n","(figure above, if pred wrong) label: 2 prediction: 0\n","(figure above, if pred wrong) label: 0 prediction: 2\n","(figure above, if pred wrong) label: 2 prediction: 0\n","(figure above, if pred wrong) label: 0 prediction: 2\n","(figure above, if pred wrong) label: 0 prediction: 2\n","(figure above, if pred wrong) label: 2 prediction: 0\n","(figure above, if pred wrong) label: 1 prediction: 1\n","(figure above, if pred wrong) label: 0 prediction: 2\n","(figure above, if pred wrong) label: 2 prediction: 0\n","(figure above, if pred wrong) label: 2 prediction: 2\n","(figure above, if pred wrong) label: 0 prediction: 0\n","Confusion Matrix:\n"," [[ 9  0  6]\n"," [11  2  5]\n"," [12  0  3]]\n","Classification Report:\n","               precision    recall  f1-score   support\n","\n","           0       0.28      0.60      0.38        15\n","           1       1.00      0.11      0.20        18\n","           2       0.21      0.20      0.21        15\n","\n","    accuracy                           0.29        48\n","   macro avg       0.50      0.30      0.26        48\n","weighted avg       0.53      0.29      0.26        48\n","\n","Accuracy Score: 0.2916666666666667\n"]}],"source":["\n","plot_wrong_prediton = False\n","model = 'xgBoost'\n","\n","if model == 'SVM':\n","    loaded_model = joblib.load('./trained_models/location_svm_model.joblib')\n","elif model == 'xgBoost':\n","    loaded_model = joblib.load('./trained_models/location_xgboost_model.joblib')\n","\n","\n","\n","y_labels = [] # labels of all testing traces\n","y_overalls = [] # final overall predictions for all testing traces\n","\n","for traceName in traceNames[221:-1]:\n","    if traceName[-5:] == '.json':\n","        # Load trace and extract features\n","        trace = Recording(path_training+'/'+ traceName, no_labels=False, mute=True)\n","        features_df = preprocess_and_extract_features(trace)\n","        features_df = features_df[features_df['moving']] # Keep only moving windows\n","    \n","        # Prepare data for classification\n","        y_trace = features_df['loc_label'] # loc labels for every window in a trace (the same loc for one trace)\n","        y_label = np.argmax(np.bincount(y_trace.astype(int))) # Squeeze the same loc labels for every window of a trace into the only label for the whole trace\n","        y_labels.append(y_label)\n","\n","        X_test = features_df.drop(['loc_label', 'Gm', 'Hm', 'Im'], axis=1)\n","        assert all(X_test['moving']), \"Not all features are extracted from moving part\"\n","        X_test = scaler.fit_transform(X_test)\n","        if model == 'xgBoost':\n","            dtest = xgb.DMatrix(X_test) # Create the XGBoost DMatrix object for the test data\n","        \n","        # Predict the location with loaded model\n","        y_pred = loaded_model.predict(dtest)\n","        y_pred = np.squeeze(y_pred)\n","        y_overall = np.argmax(np.bincount(y_pred.astype(int))) # Get the mode of predictions of every windows in a trace of the final overall predicition for the whole trace\n","        y_overalls.append(y_overall)\n","        print('(figure above, if pred wrong) label:',y_label,'prediction:',y_overall)\n","        \n","        if y_overall != y_label and plot_wrong_prediton:\n","            figure, ax = plt.subplots(4,1,figsize=(60, 5))\n","            figure.suptitle(str(trace.labels))\n","            ax[0].plot(get_lowpass(get_highpass(trace.data['ax'].values)))\n","            ax[0].plot(get_lowpass(get_highpass(trace.data['ay'].values)))\n","            ax[0].plot(get_lowpass(get_highpass(trace.data['az'].values)))\n","\n","            ax[1].plot(features_df['acc_mag'])\n","        \n","            ax[2].plot(features_df['A'])\n","            ax[2].plot(features_df['B'])\n","            ax[2].plot(features_df['C'])\n","            ax[2].legend(['A: Acc-Max_mag','B: Ratio','C: Ratio'])\n","\n","            ax[3].plot(y_pred)\n","            plt.show()\n","        \n","\n","# Convert list to array    \n","y_labels = np.array(y_labels)\n","y_overalls = np.array(y_overalls)\n","\n","# Evaluate the classifier\n","print(\"Confusion Matrix:\\n\", confusion_matrix(y_labels, y_overalls))\n","print(\"Classification Report:\\n\", classification_report(y_labels, y_overalls))\n","print(\"Accuracy Score:\", accuracy_score(y_labels, y_overalls))\n","        "]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"}},"nbformat":4,"nbformat_minor":4}
