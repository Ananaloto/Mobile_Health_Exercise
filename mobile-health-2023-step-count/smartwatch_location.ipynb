{"cells":[{"cell_type":"code","execution_count":48,"metadata":{"execution":{"iopub.execute_input":"2023-03-21T17:50:11.957682Z","iopub.status.busy":"2023-03-21T17:50:11.956849Z","iopub.status.idle":"2023-03-21T17:50:11.962109Z","shell.execute_reply":"2023-03-21T17:50:11.960868Z","shell.execute_reply.started":"2023-03-21T17:50:11.957616Z"},"pycharm":{"name":"#%%\n"},"trusted":true},"outputs":[],"source":["# This is the template for the submission. If you want, you can develop your algorithm in a regular Python script and copy the code here for submission.\n","\n","# Team members (e-mail, legi):\n","# zhisun@ethz.ch, 22-958-227\n","# enjcao@ethz.ch, 22-942-700\n","# yifzhou@ethz.ch, 22-940-381"]},{"cell_type":"code","execution_count":2,"metadata":{"collapsed":false,"execution":{"iopub.execute_input":"2023-03-21T17:50:12.356641Z","iopub.status.busy":"2023-03-21T17:50:12.355451Z","iopub.status.idle":"2023-03-21T17:50:12.363393Z","shell.execute_reply":"2023-03-21T17:50:12.362041Z","shell.execute_reply.started":"2023-03-21T17:50:12.356594Z"},"jupyter":{"outputs_hidden":false},"pycharm":{"name":"#%%\n"},"trusted":true},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import os\n","\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.svm import SVC\n","from sklearn.model_selection import train_test_split, GridSearchCV\n","from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n","from scipy.stats import entropy\n","from scipy.signal import welch\n","from scipy.fftpack import fft\n","\n","from Lilygo.Recording import Recording, data_integrity\n","from Lilygo.Dataset import Dataset\n","\n","import joblib"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Filtering and Feature Extraction"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["def get_highpass(data):\n","    filtered_data = np.zeros_like(data)  # filtered_data\n","    alpla = [1, -1.905384612118461, 0.910092542787947]\n","    beta = [0.953986986993339, -1.907503180919730, 0.953986986993339]\n","\n","    for i in range(2, len(data)):\n","        filtered_data[i] = alpla[0] * (data[i] * beta[0] + data[i-1] * beta[1] + data[i-2] * beta[2] - filtered_data[i-1] * alpla[1] - filtered_data[i-2] * alpla[2])\n","    return filtered_data\n","\n","# This funciton aims to realize a high-pass filter with cutoff frequency = 5 Hz. Because according to massive amounts of data, the general \n","# maximum frequency of human walking is about 5 Hz\n","def get_lowpass(data):\n","    filtered_data = np.zeros_like(data)  # filtered_data\n","    alpla = [1, -1.80898117793047, 0.827224480562408]\n","    beta = [0.096665967120306, -0.172688631608676, 0.095465967120306]\n","\n","    for i in range(2, len(data)):\n","        filtered_data[i] = alpla[0] * (data[i] * beta[0] + data[i-1] * beta[1] + data[i-2] * beta[2] - filtered_data[i-1] * alpla[1] - filtered_data[i-2] * alpla[2])\n","    return filtered_data\n","\n","def preprocess_and_extract_features(trace, window_size=60, sampling_rate=50):\n","    \"\"\"\n","    Preprocess the data and extract features from the 3D accelerometer, gyroscope, and magnetometer data.\n","\n","    Args:\n","    trace (Lilygo.Recording.Recording): Object containing the raw data with accelerometer data stored in lists (e.g. trace.data['ax'])\n","    window_size (int): The window size in seconds for splitting the data\n","    sampling_rate (int): The sampling rate of the data in Hz\n","\n","    Returns:\n","    pd.DataFrame: A DataFrame with the extracted features and location labels\n","    \"\"\"\n","    # Read data from trace\n","    # To-Do: filter raw data with implemented function\n","    ax = get_lowpass(get_highpass(trace.data['ax'].values))\n","    ay = get_lowpass(get_highpass(trace.data['ay'].values))\n","    az = get_lowpass(get_highpass(trace.data['az'].values))\n","    \n","    '''gx = trace.data['gx'].values\n","    gy = trace.data['gy'].values\n","    gz = trace.data['gz'].values\n","    \n","    mx = trace.data['mx'].values\n","    my = trace.data['my'].values\n","    mz = trace.data['mz'].values'''\n","    \n","    # Compute the length of each window in samples\n","    window_samples = window_size * sampling_rate\n","    \n","    # Compute the number of windows in the recording\n","    num_windows = len(ax) // window_samples\n","\n","    # Initialize lists for storing extracted features and location labels\n","    features = []\n","    loc_labels = []\n","\n","    # Helper function to compute the magnitude of a vector\n","    magnitude = lambda vec: np.sqrt(np.sum(vec**2, axis=1))\n","\n","    for i in range(num_windows):\n","       \n","        # Extract the accelerometer, gyroscope, and magnetometer data for the current window\n","        acc_data = np.array([ax[i*window_samples:(i+1)*window_samples],\n","                             ay[i*window_samples:(i+1)*window_samples],\n","                             az[i*window_samples:(i+1)*window_samples]]).T\n","        '''gyro_data = np.array([gx[i*window_samples:(i+1)*window_samples],\n","                              gy[i*window_samples:(i+1)*window_samples],\n","                              gz[i*window_samples:(i+1)*window_samples]]).T\n","        mag_data = np.array([mx[i*window_samples:(i+1)*window_samples],\n","                             my[i*window_samples:(i+1)*window_samples],\n","                             mz[i*window_samples:(i+1)*window_samples]]).T'''\n","        \n","        '''figure,ax = plt.subplots(3, 1, figsize=(10, 6))\n","        ax[0].plot(acc_data[:,0])\n","        ax[0].set_ylabel('ax')\n","        ax[1].plot(acc_data[:,1])\n","        ax[1].set_ylabel('ay')\n","        ax[2].plot(acc_data[:,2])\n","        ax[2].set_ylabel('az')'''\n","\n","        # Compute magnitudes\n","        acc_magnitude = np.sqrt(np.sum(acc_data**2, axis=1))\n","        #gyro_magnitude = magnitude(gyro_data)\n","        #mag_magnitude = magnitude(mag_data)\n","\n","        # Calculate features (according to https://www.sciencedirect.com/science/article/pii/S1574119211001222)\n","        '''ax_amp = acc_data[np.argmax(acc_data[:,0]), 0] - acc_data[np.argmin(acc_data[:,0]), 0]\n","        ay_amp = acc_data[np.argmax(acc_data[:,1]), 1] - acc_data[np.argmin(acc_data[:,1]), 1]\n","        az_amp = acc_data[np.argmax(acc_data[:,2]), 2] - acc_data[np.argmin(acc_data[:,2]), 2]'''\n","        \n","        ax_amp = abs(np.mean(acc_data[:,0]))\n","        ay_amp = abs(np.mean(acc_data[:,1]))\n","        az_amp = abs(np.mean(acc_data[:,2]))\n","        a_amp_list = [ax_amp, ay_amp, az_amp]\n","        a_amp_list.sort() # Sorting list of numbers in ascending\n","        #print('ax_amp:',ax_amp,'ay_amp:',ay_amp,'az_amp',az_amp)\n","        A = a_amp_list[2] #Feature A: the maximum amplitude among all dimensions (represents motion range for location)\n","        B = a_amp_list[2]/a_amp_list[1] # Feature B and C: ratio of the maximum amplitudes in different axes (represents DoF in movement for location)\n","        C = a_amp_list[2]/a_amp_list[0]\n","        #print('A:',A,'B:',B,'C',C)\n","\n","        # Calculate the energy and entropy of acc_mag in the frequency domain (D and F)\n","        freq, Pxx = welch(acc_magnitude, fs=sampling_rate) # use of the fast Fourier transform for the estimation of power spectra\n","        #plt.plot(freq,Pxx)\n","        \n","        D = np.max(Pxx) # Feature D: the maximum energy captured by the accelerator\n","        F = np.sum(Pxx) # Feature F: the overall energy captured by the accelerator\n","        norm_Pxx = Pxx / F # normalize the power spectrum\n","        E = entropy(norm_Pxx) # Feature E: normalized information entropy of the discrete FFT component magnitudes\n","\n","   \n","        # Append the features to the list\n","        features.append([A, B, C, D, E, F])\n","        \n","        # Calculate the timestamp for the current window as the median of the timestamps (not necessary for location)\n","        # timestamp = np.median(trace.timestamp[i*window_samples:(i+1)*window_samples]) \n","\n","        # Determine the location label for the current window based on the timestamp\n","        loc_label = trace.labels.get('board_loc')\n","\n","        # Append the label to the labels list\n","        loc_labels.append(loc_label)\n","    #plt.show()\n","\n","    # Create a DataFrame with the extracted features and location labels\n","    features_df = pd.DataFrame(features, columns=['A', 'B', 'C', 'D', 'E', 'F'])\n","    features_df['loc_label'] = loc_labels\n","    #print(features_df)\n","\n","    return features_df"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Load training traces and train"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# TODO: CHANGE THIS PATH FOR KAGGLE TO LOCAL PATH\n","path_training = '/kaggle/input/mobile-health-2023-path-detection/data/train'\n","traceNames=os.listdir(path_training)\n","\n","# Get features of training traces\n","X = []\n","y = []\n","for traceName in traceNames:\n","    if traceName[-5:] == '.json':\n","        trace = Recording(path_training+'/'+ traceName, no_labels=False, mute=True)\n","        \n","        # Get features of data\n","        features_df = preprocess_and_extract_features(trace)\n","        \n","        # Prepare data for classification\n","        X_trace = features_df.drop('loc_label', axis=1)\n","        y_trace = features_df['loc_label']\n","        X.append(X_trace)\n","        y.append(y_trace)\n","X = np.array(X)\n","y = np.array(y)\n","            \n","# Normalize the features\n","scaler = StandardScaler()\n","X_scaled = scaler.fit_transform(X)\n","\n","# Using features of all training traces as training set\n","X_train = X_scaled\n","y_train = y\n","#X_train, _, y_train, _ = train_test_split(X_scaled, y, test_size=0.01, random_state=42)\n","\n","# Check the number of classes in y_train\n","'''if len(set(y_train)) < 2:\n","    raise ValueError(\"The number of classes has to be greater than one; got %d class\" % len(set(y_train)))\n","'''\n","# Train the SVM classifier with RBF kernel\n","param_grid = {'C': [0.1, 1, 10, 100], 'gamma': [1, 0.1, 0.01, 0.001]}\n","grid = GridSearchCV(SVC(kernel='rbf'), param_grid, refit=True, verbose=2)\n","grid.fit(X_train, np.ravel(y_train))\n","\n","# Save the trained SVM model\n","joblib.dump(grid, './trained_models/location_svm_model.joblib')"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Predict on testing traces with trained model and Evaluation predictions"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Load the trained model for prediction\n","loaded_model = joblib.load('./trained_models/location_svm_model.joblib')\n","\n","# Load testing data\n","# TODO: CHANGE THIS PATH FOR KAGGLE TO LOCAL PATH\n","path_testing = '/kaggle/input/mobile-health-2023-path-detection/data/test'\n","traceNames_test = os.listdir(path_testing)\n","\n","# labels and final prediction for the whole traces\n","y_labels = []\n","y_finals = []\n","for traceName in traceNames_test[0:3]:\n","    if traceName[-5:] == '.json':\n","        trace = Recording(path_testing + '/'+ traceName, no_labels=True, mute=True)\n","        \n","        # Get features of data\n","        features_df = preprocess_and_extract_features(trace)\n","\n","        # Prepare data for classification\n","        X_test = scaler.fit_transform(features_df)\n","\n","        # Predict the location with loaded model\n","        y_pred = loaded_model.predict(X_test)\n","        y_pred = np.squeeze(y_pred)\n","        y_final = np.argmax(np.bincount(y_pred.astype(int)))\n","        \n","        y_finals.append(y_final)\n","\n","y_finals = np.array(y_finals)\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"}},"nbformat":4,"nbformat_minor":4}
